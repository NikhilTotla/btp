{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rE4MO-8bDtwD",
        "outputId": "e7c739e1-7593-44ea-f9f3-3681bb527c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/finetuning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# create a seperate folder to store everything\n",
        "!mkdir finetuning\n",
        "%cd finetuning\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdhwWCgiDFXh",
        "outputId": "2367142e-c1db-4c66-e960-28780b8f820d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/finetuning/dataset"
      ],
      "metadata": {
        "id": "eOXfyBiNdrhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/dataset /content/finetuning"
      ],
      "metadata": {
        "id": "FGi_Xmxsliyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2Rs6_WkD_gF",
        "outputId": "2317a45a-5361-4daa-b5f4-11ffefc71fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 700, done.\u001b[K\n",
            "remote: Counting objects: 100% (403/403), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 700 (delta 280), reused 341 (delta 238), pack-reused 297 (from 1)\u001b[K\n",
            "Receiving objects: 100% (700/700), 2.64 MiB | 20.94 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "/content/finetuning/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1404, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 1404 (delta 139), reused 152 (delta 124), pack-reused 1219 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1404/1404), 9.57 MiB | 14.91 MiB/s, done.\n",
            "Resolving deltas: 100% (749/749), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126 (from 1)\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 34.23 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 622, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 622 (delta 25), reused 31 (delta 16), pack-reused 576 (from 1)\u001b[K\n",
            "Receiving objects: 100% (622/622), 261.27 KiB | 7.92 MiB/s, done.\n",
            "Resolving deltas: 100% (374/374), done.\n",
            "/content/finetuning\n",
            "/content/finetuning\n"
          ]
        }
      ],
      "source": [
        "# clone the repo for running finetuning\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd ..\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd finetuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-ZrIlV_uGHj",
        "outputId": "7dba9239-9e07-453f-a28c-67642b851e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'finetuning'\n",
            "/content/finetuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "duwTvJ9xEBJ1",
        "outputId": "a41b9247-ce04-4751-bdd9-0d5ea5a58d5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 1s (70.3 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126213 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting mock\n",
            "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.92 mock-5.2.0 morfessor-2.0.6 portalocker-3.1.1 sacrebleu-2.5.1 sacremoses-0.1.1 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 tensorboardX-2.6.2.2\n",
            "Collecting fairseq-fixed\n",
            "  Downloading fairseq_fixed-0.12.3.1.tar.gz (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (3.0.12)\n",
            "Collecting hydra-core==1.3.2 (from fairseq-fixed)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting omegaconf==2.3.0 (from fairseq-fixed)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2024.11.6)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2.5.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (4.67.1)\n",
            "Collecting bitarray (from fairseq-fixed)\n",
            "  Downloading bitarray-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (1.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (24.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3.2->fairseq-fixed)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf==2.3.0->fairseq-fixed) (6.0.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq-fixed) (3.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq-fixed) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq-fixed) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq-fixed) (5.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->fairseq-fixed)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->fairseq-fixed) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->fairseq-fixed) (2.22)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fairseq-fixed) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fairseq-fixed) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fairseq-fixed) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->fairseq-fixed) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Downloading bitarray-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairseq-fixed, antlr4-python3-runtime\n",
            "  Building wheel for fairseq-fixed (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq-fixed: filename=fairseq_fixed-0.12.3.1-cp311-cp311-linux_x86_64.whl size=20557271 sha256=2d6ef62811f517e4159a63dac36a8db3644b09f2a4391cdddd6bb97b08d8088b\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/c7/c0/a3af3234ddacbe945b1e65290e6474ba35fb0b6e74b84ca1d3\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=b19dd7a3f37d6e63c593e0fa5a5e5014a76c5d292c318d1d761fd2a2e2a9e2c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built fairseq-fixed antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq-fixed\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 bitarray-3.3.1 fairseq-fixed-0.12.3.1 hydra-core-1.3.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "edcf1354a9bd4cd6bd3ed56ca92365d6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! sudo apt install tree\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "!pip install fairseq-fixed\n",
        "# Install fairseq from source\n",
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# %cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "# !pip install ./\n",
        "# ! pip install xformers\n",
        "# %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_B4OHdoz_ikR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add fairseq folder to python path\n",
        "# import os\n",
        "# os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
        "\n",
        "# add these lines in usr/local/lib/python/checkpoint\n",
        "# import argparse\n",
        "# torch.serialization.add_safe_globals([argparse.Namespace])\n"
      ],
      "metadata": {
        "id": "qYNGSXRi1pmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD2EHQdqEH70",
        "outputId": "cdd0bb6f-4550-45aa-b595-c8d4f2885991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-10 15:04:14--  https://storage.googleapis.com/samanantar-public/V0.3/models/m2m.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.188.207, 192.178.163.207, 173.194.202.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.188.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4081990185 (3.8G) [application/zip]\n",
            "Saving to: ‘m2m.zip’\n",
            "\n",
            "m2m.zip             100%[===================>]   3.80G   132MB/s    in 43s     \n",
            "\n",
            "2025-04-10 15:04:56 (91.5 MB/s) - ‘m2m.zip’ saved [4081990185/4081990185]\n",
            "\n",
            "Archive:  m2m.zip\n",
            "   creating: m2m/\n",
            "   creating: m2m/vocab/\n",
            "  inflating: m2m/vocab/vocab.SRC     \n",
            "  inflating: m2m/vocab/vocab.TGT     \n",
            "  inflating: m2m/vocab/bpe_codes.32k.SRC_TGT  \n",
            "   creating: m2m/final_bin/\n",
            "  inflating: m2m/final_bin/dict.TGT.txt  \n",
            "  inflating: m2m/final_bin/dict.SRC.txt  \n",
            "   creating: m2m/model/\n",
            "  inflating: m2m/model/checkpoint_best.pt  \n"
          ]
        }
      ],
      "source": [
        "# download the indictrans model\n",
        "\n",
        "\n",
        "# downloading the en-indic model\n",
        "# this will contain:\n",
        "# en-indic/\n",
        "# ├── final_bin                          # contains fairseq dictionaries (we will use this to binarize the new finetuning data)\n",
        "# │   ├── dict.SRC.txt\n",
        "# │   └── dict.TGT.txt\n",
        "# ├── model                              # contains model checkpoint(s)\n",
        "# │   └── checkpoint_best.pt\n",
        "# └── vocab                              # contains bpes for src and tgt (since we train seperate vocabularies) generated with subword_nmt (we will use this bpes to convert finetuning data to subwords)\n",
        "#     ├── bpe_codes.32k.SRC\n",
        "#     ├── bpe_codes.32k.TGT\n",
        "#     ├── vocab.SRC\n",
        "#     └── vocab.TGT\n",
        "\n",
        "\n",
        "\n",
        "# !wget https://storage.googleapis.com/samanantar-public/V0.3/models/indic-en.zip\n",
        "# !unzip indic-en.zip\n",
        "\n",
        "# if you want to finetune indic-en models, use the link below\n",
        "\n",
        "# !wget https://storage.googleapis.com/samanantar-public/V0.3/models/en-indic.zip\n",
        "# !unzip en-indic.zip\n",
        "\n",
        "# if you want to finetune indic-indic models, use the link below\n",
        "\n",
        "!wget https://storage.googleapis.com/samanantar-public/V0.3/models/m2m.zip\n",
        "!unzip m2m.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWQhCO6UTsbx",
        "outputId": "0bd41f87-0e93-4725-c303-fd4d2be44138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8fmtRddvndk",
        "outputId": "113d66b8-dc28-4b96-e1d4-230e44450c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj7XNBuwE0OV",
        "outputId": "98b3a156-c205-4f1b-de79-f1d640555349",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-06-09 18:50:23--  http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/indic_wat_2021.tar.gz\n",
            "Resolving lotus.kuee.kyoto-u.ac.jp (lotus.kuee.kyoto-u.ac.jp)... 130.54.208.131\n",
            "Connecting to lotus.kuee.kyoto-u.ac.jp (lotus.kuee.kyoto-u.ac.jp)|130.54.208.131|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 777928004 (742M) [application/x-gzip]\n",
            "Saving to: ‘indic_wat_2021.tar.gz’\n",
            "\n",
            "indic_wat_2021.tar. 100%[===================>] 741.89M  13.6MB/s    in 57s     \n",
            "\n",
            "2021-06-09 18:51:20 (13.1 MB/s) - ‘indic_wat_2021.tar.gz’ saved [777928004/777928004]\n",
            "\n",
            "finalrepo/\n",
            "finalrepo/README\n",
            "finalrepo/dev/\n",
            "finalrepo/dev/dev.mr\n",
            "finalrepo/dev/dev.kn\n",
            "finalrepo/dev/dev.gu\n",
            "finalrepo/dev/dev.ta\n",
            "finalrepo/dev/dev.bn\n",
            "finalrepo/dev/dev.pa\n",
            "finalrepo/dev/dev.ml\n",
            "finalrepo/dev/dev.or\n",
            "finalrepo/dev/dev.en\n",
            "finalrepo/dev/dev.hi\n",
            "finalrepo/dev/dev.te\n",
            "finalrepo/train/\n",
            "finalrepo/train/zeroshotcorpstats\n",
            "finalrepo/train/opensubtitles/\n",
            "finalrepo/train/opensubtitles/en-ta/\n",
            "finalrepo/train/opensubtitles/en-ta/train.ta\n",
            "finalrepo/train/opensubtitles/en-ta/train.en\n",
            "finalrepo/train/opensubtitles/en-te/\n",
            "finalrepo/train/opensubtitles/en-te/train.te\n",
            "finalrepo/train/opensubtitles/en-te/train.en\n",
            "finalrepo/train/opensubtitles/en-ml/\n",
            "finalrepo/train/opensubtitles/en-ml/train.ml\n",
            "finalrepo/train/opensubtitles/en-ml/train.en\n",
            "finalrepo/train/opensubtitles/en-bn/\n",
            "finalrepo/train/opensubtitles/en-bn/train.bn\n",
            "finalrepo/train/opensubtitles/en-bn/train.en\n",
            "finalrepo/train/opensubtitles/en-hi/\n",
            "finalrepo/train/opensubtitles/en-hi/train.hi\n",
            "finalrepo/train/opensubtitles/en-hi/train.en\n",
            "finalrepo/train/cvit-pib/\n",
            "finalrepo/train/cvit-pib/en-ta/\n",
            "finalrepo/train/cvit-pib/en-ta/train.ta\n",
            "finalrepo/train/cvit-pib/en-ta/train.en\n",
            "finalrepo/train/cvit-pib/en-te/\n",
            "finalrepo/train/cvit-pib/en-te/train.te\n",
            "finalrepo/train/cvit-pib/en-te/train.en\n",
            "finalrepo/train/cvit-pib/en-or/\n",
            "finalrepo/train/cvit-pib/en-or/train.or\n",
            "finalrepo/train/cvit-pib/en-or/train.en\n",
            "finalrepo/train/cvit-pib/en-ml/\n",
            "finalrepo/train/cvit-pib/en-ml/train.ml\n",
            "finalrepo/train/cvit-pib/en-ml/train.en\n",
            "finalrepo/train/cvit-pib/en-bn/\n",
            "finalrepo/train/cvit-pib/en-bn/train.bn\n",
            "finalrepo/train/cvit-pib/en-bn/train.en\n",
            "finalrepo/train/cvit-pib/en-gu/\n",
            "finalrepo/train/cvit-pib/en-gu/train.en\n",
            "finalrepo/train/cvit-pib/en-gu/train.gu\n",
            "finalrepo/train/cvit-pib/en-mr/\n",
            "finalrepo/train/cvit-pib/en-mr/train.mr\n",
            "finalrepo/train/cvit-pib/en-mr/train.en\n",
            "finalrepo/train/cvit-pib/en-pa/\n",
            "finalrepo/train/cvit-pib/en-pa/train.pa\n",
            "finalrepo/train/cvit-pib/en-pa/train.en\n",
            "finalrepo/train/cvit-pib/en-hi/\n",
            "finalrepo/train/cvit-pib/en-hi/train.hi\n",
            "finalrepo/train/cvit-pib/en-hi/train.en\n",
            "finalrepo/train/bibleuedin/\n",
            "finalrepo/train/bibleuedin/en-te/\n",
            "finalrepo/train/bibleuedin/en-te/train.te\n",
            "finalrepo/train/bibleuedin/en-te/train.en\n",
            "finalrepo/train/bibleuedin/en-ml/\n",
            "finalrepo/train/bibleuedin/en-ml/train.ml\n",
            "finalrepo/train/bibleuedin/en-ml/train.en\n",
            "finalrepo/train/bibleuedin/en-gu/\n",
            "finalrepo/train/bibleuedin/en-gu/train.en\n",
            "finalrepo/train/bibleuedin/en-gu/train.gu\n",
            "finalrepo/train/bibleuedin/en-mr/\n",
            "finalrepo/train/bibleuedin/en-mr/train.mr\n",
            "finalrepo/train/bibleuedin/en-mr/train.en\n",
            "finalrepo/train/bibleuedin/en-hi/\n",
            "finalrepo/train/bibleuedin/en-hi/train.hi\n",
            "finalrepo/train/bibleuedin/en-hi/train.en\n",
            "finalrepo/train/bibleuedin/en-kn/\n",
            "finalrepo/train/bibleuedin/en-kn/train.kn\n",
            "finalrepo/train/bibleuedin/en-kn/train.en\n",
            "finalrepo/train/iitb/\n",
            "finalrepo/train/iitb/en-hi/\n",
            "finalrepo/train/iitb/en-hi/train.hi\n",
            "finalrepo/train/iitb/en-hi/train.en\n",
            "finalrepo/train/wikimatrix/\n",
            "finalrepo/train/wikimatrix/en-ta/\n",
            "finalrepo/train/wikimatrix/en-ta/train.ta\n",
            "finalrepo/train/wikimatrix/en-ta/train.en\n",
            "finalrepo/train/wikimatrix/en-te/\n",
            "finalrepo/train/wikimatrix/en-te/train.te\n",
            "finalrepo/train/wikimatrix/en-te/train.en\n",
            "finalrepo/train/wikimatrix/en-ml/\n",
            "finalrepo/train/wikimatrix/en-ml/train.ml\n",
            "finalrepo/train/wikimatrix/en-ml/train.en\n",
            "finalrepo/train/wikimatrix/en-bn/\n",
            "finalrepo/train/wikimatrix/en-bn/train.bn\n",
            "finalrepo/train/wikimatrix/en-bn/train.en\n",
            "finalrepo/train/wikimatrix/en-mr/\n",
            "finalrepo/train/wikimatrix/en-mr/train.mr\n",
            "finalrepo/train/wikimatrix/en-mr/train.en\n",
            "finalrepo/train/wikimatrix/en-hi/\n",
            "finalrepo/train/wikimatrix/en-hi/train.hi\n",
            "finalrepo/train/wikimatrix/en-hi/train.en\n",
            "finalrepo/train/alt/\n",
            "finalrepo/train/alt/en-bn/\n",
            "finalrepo/train/alt/en-bn/train.bn\n",
            "finalrepo/train/alt/en-bn/train.en\n",
            "finalrepo/train/alt/en-hi/\n",
            "finalrepo/train/alt/en-hi/train.hi\n",
            "finalrepo/train/alt/en-hi/train.en\n",
            "finalrepo/train/pmi/\n",
            "finalrepo/train/pmi/en-ta/\n",
            "finalrepo/train/pmi/en-ta/train.ta\n",
            "finalrepo/train/pmi/en-ta/train.en\n",
            "finalrepo/train/pmi/en-te/\n",
            "finalrepo/train/pmi/en-te/train.te\n",
            "finalrepo/train/pmi/en-te/train.en\n",
            "finalrepo/train/pmi/en-or/\n",
            "finalrepo/train/pmi/en-or/train.or\n",
            "finalrepo/train/pmi/en-or/train.en\n",
            "finalrepo/train/pmi/en-ml/\n",
            "finalrepo/train/pmi/en-ml/train.ml\n",
            "finalrepo/train/pmi/en-ml/train.en\n",
            "finalrepo/train/pmi/en-bn/\n",
            "finalrepo/train/pmi/en-bn/train.bn\n",
            "finalrepo/train/pmi/en-bn/train.en\n",
            "finalrepo/train/pmi/en-gu/\n",
            "finalrepo/train/pmi/en-gu/train.en\n",
            "finalrepo/train/pmi/en-gu/train.gu\n",
            "finalrepo/train/pmi/en-mr/\n",
            "finalrepo/train/pmi/en-mr/train.mr\n",
            "finalrepo/train/pmi/en-mr/train.en\n",
            "finalrepo/train/pmi/en-pa/\n",
            "finalrepo/train/pmi/en-pa/train.pa\n",
            "finalrepo/train/pmi/en-pa/train.en\n",
            "finalrepo/train/pmi/en-hi/\n",
            "finalrepo/train/pmi/en-hi/train.hi\n",
            "finalrepo/train/pmi/en-hi/train.en\n",
            "finalrepo/train/pmi/en-kn/\n",
            "finalrepo/train/pmi/en-kn/train.kn\n",
            "finalrepo/train/pmi/en-kn/train.en\n",
            "finalrepo/train/wikititles/\n",
            "finalrepo/train/wikititles/en-ta/\n",
            "finalrepo/train/wikititles/en-ta/train.ta\n",
            "finalrepo/train/wikititles/en-ta/train.en\n",
            "finalrepo/train/wikititles/en-gu/\n",
            "finalrepo/train/wikititles/en-gu/train.en\n",
            "finalrepo/train/wikititles/en-gu/train.gu\n",
            "finalrepo/train/mtenglish2odia/\n",
            "finalrepo/train/mtenglish2odia/en-or/\n",
            "finalrepo/train/mtenglish2odia/en-or/train.or\n",
            "finalrepo/train/mtenglish2odia/en-or/train.en\n",
            "finalrepo/train/urst/\n",
            "finalrepo/train/urst/en-gu/\n",
            "finalrepo/train/urst/en-gu/train.en\n",
            "finalrepo/train/urst/en-gu/train.gu\n",
            "finalrepo/train/jw/\n",
            "finalrepo/train/jw/en-ta/\n",
            "finalrepo/train/jw/en-ta/train.ta\n",
            "finalrepo/train/jw/en-ta/train.en\n",
            "finalrepo/train/jw/en-te/\n",
            "finalrepo/train/jw/en-te/train.te\n",
            "finalrepo/train/jw/en-te/train.en\n",
            "finalrepo/train/jw/en-ml/\n",
            "finalrepo/train/jw/en-ml/train.ml\n",
            "finalrepo/train/jw/en-ml/train.en\n",
            "finalrepo/train/jw/en-bn/\n",
            "finalrepo/train/jw/en-bn/train.bn\n",
            "finalrepo/train/jw/en-bn/train.en\n",
            "finalrepo/train/jw/en-gu/\n",
            "finalrepo/train/jw/en-gu/train.en\n",
            "finalrepo/train/jw/en-gu/train.gu\n",
            "finalrepo/train/jw/en-mr/\n",
            "finalrepo/train/jw/en-mr/train.mr\n",
            "finalrepo/train/jw/en-mr/train.en\n",
            "finalrepo/train/jw/en-pa/\n",
            "finalrepo/train/jw/en-pa/train.pa\n",
            "finalrepo/train/jw/en-pa/train.en\n",
            "finalrepo/train/jw/en-hi/\n",
            "finalrepo/train/jw/en-hi/train.hi\n",
            "finalrepo/train/jw/en-hi/train.en\n",
            "finalrepo/train/jw/en-kn/\n",
            "finalrepo/train/jw/en-kn/train.kn\n",
            "finalrepo/train/jw/en-kn/train.en\n",
            "finalrepo/train/nlpc/\n",
            "finalrepo/train/nlpc/en-ta/\n",
            "finalrepo/train/nlpc/en-ta/train.ta\n",
            "finalrepo/train/nlpc/en-ta/train.en\n",
            "finalrepo/train/get_zero_shot_pairs.py\n",
            "finalrepo/train/ufal/\n",
            "finalrepo/train/ufal/en-ta/\n",
            "finalrepo/train/ufal/en-ta/train.ta\n",
            "finalrepo/train/ufal/en-ta/train.en\n",
            "finalrepo/train/odiencorp/\n",
            "finalrepo/train/odiencorp/en-or/\n",
            "finalrepo/train/odiencorp/en-or/train.or\n",
            "finalrepo/train/odiencorp/en-or/train.en\n",
            "finalrepo/train/tanzil/\n",
            "finalrepo/train/tanzil/en-ta/\n",
            "finalrepo/train/tanzil/en-ta/train.ta\n",
            "finalrepo/train/tanzil/en-ta/train.en\n",
            "finalrepo/train/tanzil/en-ml/\n",
            "finalrepo/train/tanzil/en-ml/train.ml\n",
            "finalrepo/train/tanzil/en-ml/train.en\n",
            "finalrepo/train/tanzil/en-bn/\n",
            "finalrepo/train/tanzil/en-bn/train.bn\n",
            "finalrepo/train/tanzil/en-bn/train.en\n",
            "finalrepo/train/tanzil/en-hi/\n",
            "finalrepo/train/tanzil/en-hi/train.hi\n",
            "finalrepo/train/tanzil/en-hi/train.en\n",
            "finalrepo/train/ted2020/\n",
            "finalrepo/train/ted2020/en-ta/\n",
            "finalrepo/train/ted2020/en-ta/train.ta\n",
            "finalrepo/train/ted2020/en-ta/train.en\n",
            "finalrepo/train/ted2020/en-te/\n",
            "finalrepo/train/ted2020/en-te/train.te\n",
            "finalrepo/train/ted2020/en-te/train.en\n",
            "finalrepo/train/ted2020/en-ml/\n",
            "finalrepo/train/ted2020/en-ml/train.ml\n",
            "finalrepo/train/ted2020/en-ml/train.en\n",
            "finalrepo/train/ted2020/en-bn/\n",
            "finalrepo/train/ted2020/en-bn/train.bn\n",
            "finalrepo/train/ted2020/en-bn/train.en\n",
            "finalrepo/train/ted2020/en-gu/\n",
            "finalrepo/train/ted2020/en-gu/train.en\n",
            "finalrepo/train/ted2020/en-gu/train.gu\n",
            "finalrepo/train/ted2020/en-mr/\n",
            "finalrepo/train/ted2020/en-mr/train.mr\n",
            "finalrepo/train/ted2020/en-mr/train.en\n",
            "finalrepo/train/ted2020/en-pa/\n",
            "finalrepo/train/ted2020/en-pa/train.pa\n",
            "finalrepo/train/ted2020/en-pa/train.en\n",
            "finalrepo/train/ted2020/en-hi/\n",
            "finalrepo/train/ted2020/en-hi/train.hi\n",
            "finalrepo/train/ted2020/en-hi/train.en\n",
            "finalrepo/train/ted2020/en-kn/\n",
            "finalrepo/train/ted2020/en-kn/train.kn\n",
            "finalrepo/train/ted2020/en-kn/train.en\n",
            "finalrepo/test/\n",
            "finalrepo/test/test.gu\n",
            "finalrepo/test/test.fm.prob\n",
            "finalrepo/test/test.kn\n",
            "finalrepo/test/test.ta\n",
            "finalrepo/test/cached_lm_test.en\n",
            "finalrepo/test/test.pa\n",
            "finalrepo/test/test.bn\n",
            "finalrepo/test/test.hi\n",
            "finalrepo/test/test.ml\n",
            "finalrepo/test/test.or\n",
            "finalrepo/test/test.mr\n",
            "finalrepo/test/test.en\n",
            "finalrepo/test/test.te\n"
          ]
        }
      ],
      "source": [
        "# In this example, we will finetuning on cvit-pib corpus which is part of the WAT2021 training dataset.\n",
        "\n",
        "# Lets first download the full wat2021 training data (cvit-pib is a part of this big training set)\n",
        "# ***Note***: See the next section to mine for mining indic to indic data from english centric WAT data. This dataset can be used to finetune indic2indic model\n",
        "!wget http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/indic_wat_2021.tar.gz\n",
        "!tar -xzvf indic_wat_2021.tar.gz\n",
        "# all train sets will now be in wat2021/train\n",
        "!mv finalrepo wat2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSoZDR3fHpUk",
        "outputId": "11bd057b-d1b0-45b8-feac-85b3e900104e",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘wat2021-indic2indic’: File exists\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "bn hi\n",
            "bn gu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [03:46<03:46, 226.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "hi gu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [06:05<00:00, 182.80s/it]\n"
          ]
        }
      ],
      "source": [
        "# this cell is for mining indic to indic data from a english centric corpus. This data can then be used to our finetune indic2indic model\n",
        "\n",
        "# Mining Indic to Indic pairs from english centric corpus\n",
        "# The `extract_non_english_pairs` in `scripts/extract_non_english_pairs.py` can be used to mine indic to indic pairs from english centric corpus.\n",
        "\n",
        "# As described in the paper (section 2.5) , we use a very strict deduplication criterion to avoid the creation of very similar parallel sentences.\n",
        "# For example, if an en sentence is aligned to M hi sentences and N ta sentences, then we would get MN hi-ta pairs. However, these pairs would be very similar and not contribute much to the training process.\n",
        "# Hence, we retain only 1 randomly chosen pair out of these MN pairs.\n",
        "\n",
        "!mkdir wat2021-indic2indic\n",
        "\n",
        "from indicTrans.scripts.extract_non_english_pairs import extract_non_english_pairs\n",
        "\n",
        "\"\"\"\n",
        "extract_non_english_pairs(indir, outdir, LANGS)\n",
        "\n",
        "    Extracts non-english pair parallel corpora\n",
        "    indir: contains english centric data in the following form:\n",
        "            - directory named en-xx for language xx\n",
        "            - each directory contains a train.en and train.xx\n",
        "    outdir: output directory to store mined data for each pair.\n",
        "            One directory is created for each pair.\n",
        "    LANGS: list of languages in the corpus (other than English).\n",
        "            The language codes must correspond to the ones used in the\n",
        "            files and directories in indir. Prefarably, sort the languages\n",
        "            in this list in alphabetic order. outdir will contain data for xx-yy,\n",
        "            but not for yy-xx, so it will be convenient to have this list in sorted order.\n",
        "\"\"\"\n",
        "# here we are using three langs to mine bn-hi, hi-gu and gu-bn pairs from wat2021/cvit-pib en-X data\n",
        "# you should see the following files after running the code below\n",
        "#  wat2021-indic2indic\n",
        "#  ├── bn-gu\n",
        "#  │   ├── train.bn\n",
        "#  │   └── train.gu\n",
        "#  ├── bn-hi\n",
        "#  │   ├── train.bn\n",
        "#  │   └── train.hi\n",
        "#  └── hi-gu\n",
        "#      ├── train.gu\n",
        "#      └── train.hi\n",
        "\n",
        "# NOTE: Make sure to dedup the output text files and remove any overlaps with test sets before finetuning\n",
        "#       Both of the above are implemented in scripts/remove_train_devtest_overlaps.py -> remove_train_devtest_overlaps(train_dir, devtest_dir, many2many=True)\n",
        "\n",
        "extract_non_english_pairs('wat2021/train/cvit-pib', 'wat2021-indic2indic', ['bn', 'hi', 'gu'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys_QURP3Sx7G",
        "outputId": "d41f5baa-e700-4e07-93cd-b23b08122dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/finetuning/indicTrans\n"
          ]
        }
      ],
      "source": [
        "# wat2021\n",
        "# ├── dev                    # contains Wat2021 dev data\n",
        "# │   ├── dev.bn\n",
        "# │   ├── dev.en\n",
        "# │   ├── dev.gu\n",
        "# │   ├── dev.hi\n",
        "# │   ├── dev.kn\n",
        "# │   ├── dev.ml\n",
        "# │   ├── dev.mr\n",
        "# │   ├── dev.or\n",
        "# │   ├── dev.pa\n",
        "# │   ├── dev.ta\n",
        "# │   └── dev.te\n",
        "# ├── README\n",
        "# ├── test                  # contains Wat2021 test data\n",
        "# │   ├── test.bn\n",
        "# │   ├── test.en\n",
        "# │   ├── test.gu\n",
        "# │   ├── test.hi\n",
        "# │   ├── test.kn\n",
        "# │   ├── test.ml\n",
        "# │   ├── test.mr\n",
        "# │   ├── test.or\n",
        "# │   ├── test.pa\n",
        "# │   ├── test.ta\n",
        "# │   └── test.te\n",
        "# └── train                 # contains WAT2021 train data which has lot of corpuses (alt, bible, Jw300, etc)\n",
        "#     ├── alt/\n",
        "#     ├── bibleuedin/\n",
        "#     ├── iitb/\n",
        "#     ├── jw/\n",
        "#     ├── mtenglish2odia/\n",
        "#     ├── nlpc/\n",
        "#     ├── odiencorp/\n",
        "#     ├── opensubtitles/\n",
        "#     ├── pmi/\n",
        "#     ├── tanzil/\n",
        "#     ├── ted2020/\n",
        "#     ├── ufal/\n",
        "#     ├── urst/\n",
        "#     ├── wikimatrix/\n",
        "#     ├── wikititles/\n",
        "#     └──  cvit-pib\n",
        "#         ├── en-bn         # within a train corpus folder the files are arranged in {src_lang}-{tgt_lang}/train.{src_lang}, train.{tgt_lang}\n",
        "#         │   ├── train.bn\n",
        "#         │   └── train.en\n",
        "#         ├── en-gu\n",
        "#         │   ├── train.en\n",
        "#         │   └── train.gu\n",
        "#         ├── en-hi\n",
        "#         │   ├── train.en\n",
        "#         │   └── train.hi\n",
        "#         ├── en-ml\n",
        "#         │   ├── train.en\n",
        "#         │   └── train.ml\n",
        "#         ├── en-mr\n",
        "#         │   ├── train.en\n",
        "#         │   └── train.mr\n",
        "#         ├── en-or\n",
        "#         │   ├── train.en\n",
        "#         │   └── train.or\n",
        "#         ├── en-pa\n",
        "#         │   ├── train.en\n",
        "#         │   └── train.pa\n",
        "#         ├── en-ta\n",
        "#         │   ├── train.en\n",
        "#         │   └── train.ta\n",
        "#         └── en-te\n",
        "#             ├── train.en\n",
        "#             └── train.te\n",
        "\n",
        "\n",
        "\n",
        "# instead of using all the data for this example, we will mainly use the cvit-pib corpus from wat2021 train set\n",
        "# for dev and test set, we will use the dev and test provided by wat2021\n",
        "\n",
        "# In case, you want to finetune on all these corpuses, you would need to merge all the training data into one folder and remove duplicate train sentence pairs.\n",
        "# To do this, refer to this gist: https://gist.github.com/gowtham1997/2524f8e9559cff586d1f935e621fc598\n",
        "\n",
        "\n",
        "# copy everything to a dataset folder\n",
        "!mkdir -p dataset/train\n",
        "! cp -r wat2021/train/cvit-pib/* dataset/train\n",
        "! cp -r wat2021/dev dataset\n",
        "! cp -r wat2021/test dataset\n",
        "\n",
        "\n",
        "# lets cd to indicTrans\n",
        "%cd indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd indicTrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXDnWCulWayi",
        "outputId": "7f0b08eb-3458-491c-e70a-9aa844518fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning/indicTrans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yPTbM_clKfI",
        "outputId": "ce6d5dc5-7919-4dbf-84e4-51ca4112c07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../dataset\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "exp_dir=../dataset\n",
        "src_lang=indic   #change\n",
        "tgt_lang=en        #change\n",
        "\n",
        "# change this to indic-en, if you have downloaded the indic-en dir or m2m if you have downloaded the indic2indic model\n",
        "download_dir=../m2m\n",
        "\n",
        "train_data_dir=$exp_dir/train\n",
        "dev_data_dir=$exp_dir/dev\n",
        "test_data_dir=$exp_dir/test\n",
        "echo $exp_dir\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/finetuning/m2m/vocab"
      ],
      "metadata": {
        "id": "Nbfv-27KEPfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/vocab /content/finetuning/m2m"
      ],
      "metadata": {
        "id": "R9gDTEuHEYbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhwUXyYVXrOY",
        "outputId": "960f9009-5ed9-4e75-c1b4-3be873015739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment ../dataset on indic to en\n",
            "Applying normalization and script conversion for train hi\n",
            "100% 266545/266545 [00:29<00:00, 9097.04it/s]\n",
            "100% 266545/266545 [01:00<00:00, 4409.29it/s]\n",
            "Number of sentences in train hi: 266545\n",
            "Applying normalization and script conversion for dev hi\n",
            "100% 1000/1000 [00:00<00:00, 6346.40it/s]\n",
            "100% 1000/1000 [00:00<00:00, 5730.22it/s]\n",
            "Number of sentences in dev hi: 1000\n",
            "Applying normalization and script conversion for test hi\n",
            "100% 2390/2390 [00:00<00:00, 8623.23it/s]\n",
            "100% 2390/2390 [00:00<00:00, 5834.60it/s]\n",
            "Number of sentences in test hi: 2390\n",
            "\n",
            "../dataset/data/train.SRC\n",
            "../dataset/data/train.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "../dataset/norm/hi-en/train.hi\n",
            "../dataset/norm/hi-en/train.en\n",
            " 36% 4/11 [00:00<00:00, 19.36it/s]src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 53.13it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "../dataset/norm/hi-en/train.hi\n",
            " 36% 4/11 [00:00<00:00,  9.92it/s]src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 27.25it/s]\n",
            "\n",
            "../dataset/data/dev.SRC\n",
            "../dataset/data/dev.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "../dataset/norm/hi-en/dev.hi\n",
            "../dataset/norm/hi-en/dev.en\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 2788.93it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "../dataset/norm/hi-en/dev.hi\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 8779.70it/s]\n",
            "\n",
            "../dataset/data/test.SRC\n",
            "../dataset/data/test.TGT\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "../dataset/norm/hi-en/test.hi\n",
            "../dataset/norm/hi-en/test.en\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 2556.37it/s]\n",
            "  0% 0/11 [00:00<?, ?it/s]src: as, tgt:en\n",
            "src: bn, tgt:en\n",
            "src: gu, tgt:en\n",
            "src: hi, tgt:en\n",
            "../dataset/norm/hi-en/test.hi\n",
            "src: kn, tgt:en\n",
            "src: ml, tgt:en\n",
            "src: mr, tgt:en\n",
            "src: or, tgt:en\n",
            "src: pa, tgt:en\n",
            "src: ta, tgt:en\n",
            "src: te, tgt:en\n",
            "100% 11/11 [00:00<00:00, 5032.43it/s]\n",
            "Applying bpe to the new finetuning data\n",
            "train\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "dev\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "test\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Adding language tags\n",
            "266545it [00:02, 123074.44it/s]\n",
            "1000it [00:00, 148728.91it/s]\n",
            "2390it [00:00, 167671.14it/s]\n",
            "Binarizing data. This will take some time depending on the size of finetuning data\n",
            "2025-04-10 15:17:45.448083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744298265.469274    7020 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744298265.476320    7020 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='SRC', target_lang='TGT', trainpref='../dataset/final/train', validpref='../dataset/final/dev', testpref='../dataset/final/test', align_suffix=None, destdir='../dataset/final_bin', thresholdtgt=5, thresholdsrc=5, tgtdict='../m2m/final_bin/dict.TGT.txt', srcdict='../m2m/final_bin/dict.SRC.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=2, dict_only=False)\n",
            "INFO:fairseq_cli.preprocess:[SRC] Dictionary: 44840 types\n",
            "INFO:fairseq_cli.preprocess:[SRC] ../dataset/final/train.SRC: 266545 sents, 10943865 tokens, 27.1% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[SRC] Dictionary: 44840 types\n",
            "INFO:fairseq_cli.preprocess:[SRC] ../dataset/final/dev.SRC: 1000 sents, 24973 tokens, 26.4% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[SRC] Dictionary: 44840 types\n",
            "INFO:fairseq_cli.preprocess:[SRC] ../dataset/final/test.SRC: 2390 sents, 59750 tokens, 25.9% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[TGT] Dictionary: 44840 types\n",
            "INFO:fairseq_cli.preprocess:[TGT] ../dataset/final/train.TGT: 266545 sents, 8927753 tokens, 2.9% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[TGT] Dictionary: 44840 types\n",
            "INFO:fairseq_cli.preprocess:[TGT] ../dataset/final/dev.TGT: 1000 sents, 20291 tokens, 2.4% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[TGT] Dictionary: 44840 types\n",
            "INFO:fairseq_cli.preprocess:[TGT] ../dataset/final/test.TGT: 2390 sents, 47616 tokens, 2.18% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:Wrote preprocessed data to ../dataset/final_bin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# all the data preparation happens in this cell\n",
        "%%shell\n",
        "\n",
        "exp_dir=../dataset\n",
        "src_lang=indic\n",
        "tgt_lang=en\n",
        "\n",
        "# change this to indic-en, if you have downloaded the indic-en dir or m2m if you have downloaded the indic2indic model\n",
        "download_dir=../m2m\n",
        "\n",
        "train_data_dir=$exp_dir/train\n",
        "dev_data_dir=$exp_dir/dev\n",
        "test_data_dir=$exp_dir/test\n",
        "\n",
        "\n",
        "echo \"Running experiment ${exp_dir} on ${src_lang} to ${tgt_lang}\"\n",
        "\n",
        "\n",
        "train_processed_dir=$exp_dir/data\n",
        "devtest_processed_dir=$exp_dir/data\n",
        "\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "mkdir -p $train_processed_dir\n",
        "mkdir -p $devtest_processed_dir\n",
        "mkdir -p $out_data_dir\n",
        "\n",
        "# indic languages.\n",
        "# cvit-pib corpus does not have as (assamese) and kn (kannada), hence its not part of this list\n",
        "langs=(hi)\n",
        "\n",
        "for lang in ${langs[@]};do\n",
        "\tif [ $src_lang == en ]; then\n",
        "\t\ttgt_lang=$lang\n",
        "\telse\n",
        "\t\tsrc_lang=$lang\n",
        "\tfi\n",
        "\n",
        "\ttrain_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\tdevtest_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\tmkdir -p $train_norm_dir\n",
        "\tmkdir -p $devtest_norm_dir\n",
        "\n",
        "\n",
        "    # preprocessing pretokenizes the input (we use moses tokenizer for en and indicnlp lib for indic languages)\n",
        "    # after pretokenization, we use indicnlp to transliterate all the indic data to devnagiri script\n",
        "\n",
        "\t# train preprocessing\n",
        "\ttrain_infname_src=$train_data_dir/${lang}-en/train.$src_lang\n",
        "\ttrain_infname_tgt=$train_data_dir/${lang}-en/train.$tgt_lang\n",
        "\ttrain_outfname_src=$train_norm_dir/train.$src_lang\n",
        "\ttrain_outfname_tgt=$train_norm_dir/train.$tgt_lang\n",
        "\techo \"Applying normalization and script conversion for train $lang\"\n",
        "\tinput_size=`python scripts/preprocess_translate.py $train_infname_src $train_outfname_src $src_lang true`\n",
        "\tinput_size=`python scripts/preprocess_translate.py $train_infname_tgt $train_outfname_tgt $tgt_lang true`\n",
        "\techo \"Number of sentences in train $lang: $input_size\"\n",
        "\n",
        "\t# dev preprocessing\n",
        "\tdev_infname_src=$dev_data_dir/dev.$src_lang\n",
        "\tdev_infname_tgt=$dev_data_dir/dev.$tgt_lang\n",
        "\tdev_outfname_src=$devtest_norm_dir/dev.$src_lang\n",
        "\tdev_outfname_tgt=$devtest_norm_dir/dev.$tgt_lang\n",
        "\techo \"Applying normalization and script conversion for dev $lang\"\n",
        "\tinput_size=`python scripts/preprocess_translate.py $dev_infname_src $dev_outfname_src $src_lang true`\n",
        "\tinput_size=`python scripts/preprocess_translate.py $dev_infname_tgt $dev_outfname_tgt $tgt_lang true`\n",
        "\techo \"Number of sentences in dev $lang: $input_size\"\n",
        "\n",
        "\t# test preprocessing\n",
        "\ttest_infname_src=$test_data_dir/test.$src_lang\n",
        "\ttest_infname_tgt=$test_data_dir/test.$tgt_lang\n",
        "\ttest_outfname_src=$devtest_norm_dir/test.$src_lang\n",
        "\ttest_outfname_tgt=$devtest_norm_dir/test.$tgt_lang\n",
        "\techo \"Applying normalization and script conversion for test $lang\"\n",
        "\tinput_size=`python scripts/preprocess_translate.py $test_infname_src $test_outfname_src $src_lang true`\n",
        "\tinput_size=`python scripts/preprocess_translate.py $test_infname_tgt $test_outfname_tgt $tgt_lang true`\n",
        "\techo \"Number of sentences in test $lang: $input_size\"\n",
        "done\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now that we have preprocessed all the data, we can now merge these different text files into one\n",
        "# ie. for en-as, we have train.en and corresponding train.as, similarly for en-bn, we have train.en and corresponding train.bn\n",
        "# now we will concatenate all this into en-X where train.SRC will have all the en (src) training data and train.TGT will have all the concatenated indic lang data\n",
        "\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'train'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'dev'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'test'\n",
        "\n",
        "# use the vocab from downloaded dir\n",
        "cp -r $download_dir/vocab $exp_dir\n",
        "\n",
        "\n",
        "echo \"Applying bpe to the new finetuning data\"\n",
        "bash apply_single_bpe_traindevtest_notag.sh $exp_dir\n",
        "\n",
        "mkdir -p $exp_dir/final\n",
        "\n",
        "# We also add special tags to indicate the source and target language in the inputs\n",
        "#  Eg: to translate a sentence from english to hindi , the input would be   __src__en__   __tgt__hi__ <en bpe tokens>\n",
        "\n",
        "echo \"Adding language tags\"\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'train'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'dev'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'test'\n",
        "\n",
        "\n",
        "\n",
        "data_dir=$exp_dir/final\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "rm -rf $out_data_dir\n",
        "\n",
        "# binarizing the new data (train, dev and test) using dictionary from the download dir\n",
        "\n",
        " num_workers=`python -c \"import multiprocessing; print(multiprocessing.cpu_count())\"`\n",
        "\n",
        "data_dir=$exp_dir/final\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "# rm -rf $out_data_dir\n",
        "\n",
        "echo \"Binarizing data. This will take some time depending on the size of finetuning data\"\n",
        "fairseq-preprocess --source-lang SRC --target-lang TGT \\\n",
        " --trainpref $data_dir/train --validpref $data_dir/dev --testpref $data_dir/test \\\n",
        " --destdir $out_data_dir --workers $num_workers \\\n",
        " --srcdict $download_dir/final_bin/dict.SRC.txt --tgtdict $download_dir/final_bin/dict.TGT.txt --thresholdtgt 5 --thresholdsrc 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/finetuning/m2m/vocab/bpe_codes.32k.SRC /content/finetuning/m2m/vocab/bpe_codes.32k.TGT"
      ],
      "metadata": {
        "id": "LW592WMLd9jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmiJZ5oLfQXq",
        "outputId": "b24c7408-ac88-4199-89d7-5cb5bd178ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz6tzbe2tcs7",
        "outputId": "b3a0499d-8d30-411e-80c5-7f3ade31ee15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-09 13:02:36.144679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744203756.180655   17525 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744203756.191600   17525 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-09 13:02:39 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  deprecation_warning(message=message)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  self.delegate = real_initialize(\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
            "  deprecation_warning(message=message)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  deprecation_warning(\n",
            "sys:1: UserWarning: \n",
            "'config' is validated against ConfigStore schema with the same name.\n",
            "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/compose.py:56: UserWarning: \n",
            "The strict flag in the compose API is deprecated.\n",
            "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
            "\n",
            "  deprecation_warning(\n",
            "2025-04-09 13:02:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../dataset/tensorboard-wandb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'model_configs', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 256, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 256, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 30, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '../dataset/model', 'restore_file': '../en-indic/model/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='../dataset/tensorboard-wandb', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir='model_configs', empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=256, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=256, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_4x', max_epoch=0, max_update=30, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='../dataset/model', restore_file='../en-indic/model/checkpoint_best.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='../dataset/final_bin', source_lang='SRC', target_lang='TGT', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, max_source_positions=210, max_target_positions=210, dropout=0.2, no_seed_provided=False, encoder_embed_dim=1536, encoder_ffn_embed_dim=4096, encoder_attention_heads=16, encoder_normalize_before=False, decoder_embed_dim=1536, decoder_ffn_embed_dim=4096, decoder_attention_heads=16, encoder_embed_path=None, encoder_layers=6, encoder_learned_pos=False, decoder_embed_path=None, decoder_layers=6, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1536, decoder_input_dim=1536, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_4x'), 'task': {'_name': 'translation', 'data': '../dataset/final_bin', 'source_lang': 'SRC', 'target_lang': 'TGT', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 210, 'max_target_positions': 210, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2025-04-09 13:02:44 | INFO | fairseq.tasks.translation | [SRC] dictionary: 44840 types\n",
            "2025-04-09 13:02:44 | INFO | fairseq.tasks.translation | [TGT] dictionary: 44840 types\n",
            "2025-04-09 13:02:52 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(44840, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(44840, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=1536, out_features=44840, bias=False)\n",
            "  )\n",
            ")\n",
            "2025-04-09 13:02:52 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2025-04-09 13:02:52 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2025-04-09 13:02:52 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2025-04-09 13:02:52 | INFO | fairseq_cli.train | num. shared model params: 527,757,312 (num. trained: 527,757,312)\n",
            "2025-04-09 13:02:52 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2025-04-09 13:02:52 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: ../dataset/final_bin/valid.SRC-TGT.SRC\n",
            "2025-04-09 13:02:52 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: ../dataset/final_bin/valid.SRC-TGT.TGT\n",
            "2025-04-09 13:02:52 | INFO | fairseq.tasks.translation | ../dataset/final_bin valid SRC-TGT 1000 examples\n",
            "2025-04-09 13:02:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2025-04-09 13:02:54 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.741 GB ; name = Tesla T4                                \n",
            "2025-04-09 13:02:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2025-04-09 13:02:54 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2025-04-09 13:02:54 | INFO | fairseq_cli.train | max tokens per device = 256 and max sentences per device = None\n",
            "2025-04-09 13:02:54 | INFO | fairseq.trainer | Preparing to load checkpoint ../en-indic/model/checkpoint_best.pt\n",
            "2025-04-09 13:02:54 | INFO | fairseq.trainer | No existing checkpoint found ../en-indic/model/checkpoint_best.pt\n",
            "2025-04-09 13:02:54 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2025-04-09 13:02:54 | INFO | fairseq.data.data_utils | loaded 266,545 examples from: ../dataset/final_bin/train.SRC-TGT.SRC\n",
            "2025-04-09 13:02:54 | INFO | fairseq.data.data_utils | loaded 266,545 examples from: ../dataset/final_bin/train.SRC-TGT.TGT\n",
            "2025-04-09 13:02:54 | INFO | fairseq.tasks.translation | ../dataset/final_bin train SRC-TGT 266545 examples\n",
            "2025-04-09 13:02:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-04-09 13:02:54 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2025-04-09 13:02:54 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2025-04-09 13:02:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2025-04-09 13:02:54 | WARNING | fairseq.tasks.fairseq_task | 36,883 samples have invalid sizes and will be skipped, max_positions=(210, 210), first few sample ids=[263780, 73902, 68829, 153383, 7599, 203436, 174945, 130871, 216776, 167928]\n",
            "2025-04-09 13:02:56 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
            "2025-04-09 13:02:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-04-09 13:02:56 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2025-04-09 13:02:56 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2025-04-09 13:02:56 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2025-04-09 13:02:56 | WARNING | fairseq.tasks.fairseq_task | 4 samples have invalid sizes and will be skipped, max_positions=(210, 210), first few sample ids=[852, 682, 834, 497]\n",
            "2025-04-09 13:02:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65400\n",
            "epoch 001:   0% 0/65400 [00:00<?, ?it/s]2025-04-09 13:02:58 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2025-04-09 13:02:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.11/dist-packages/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n",
            "epoch 001:   0% 2/65400 [00:02<18:06:28,  1.00it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 001:   0% 29/65400 [00:12<6:05:21,  2.98it/s]2025-04-09 13:03:10 | INFO | fairseq_cli.train | Stopping training due to num_updates: 30 >= max_update: 30\n",
            "2025-04-09 13:03:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-04-09 13:03:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/395 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 2/395 [00:00<00:23, 16.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 7/395 [00:00<00:12, 30.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 12/395 [00:00<00:10, 35.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 16/395 [00:00<00:10, 37.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 21/395 [00:00<00:09, 38.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 26/395 [00:00<00:09, 40.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 31/395 [00:00<00:08, 40.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 36/395 [00:00<00:09, 39.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  10% 41/395 [00:01<00:08, 40.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 46/395 [00:01<00:08, 40.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 51/395 [00:01<00:08, 41.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 56/395 [00:01<00:08, 41.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 61/395 [00:01<00:07, 42.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 66/395 [00:01<00:07, 43.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 71/395 [00:01<00:07, 43.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  19% 76/395 [00:01<00:07, 43.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 81/395 [00:02<00:07, 42.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 86/395 [00:02<00:07, 42.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 91/395 [00:02<00:07, 42.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 96/395 [00:02<00:06, 43.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 101/395 [00:02<00:06, 43.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 106/395 [00:02<00:06, 42.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 111/395 [00:02<00:06, 42.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 116/395 [00:02<00:06, 43.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 121/395 [00:02<00:06, 44.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 126/395 [00:03<00:06, 43.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 131/395 [00:03<00:06, 42.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 136/395 [00:03<00:05, 43.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 141/395 [00:03<00:05, 44.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 146/395 [00:03<00:05, 42.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 151/395 [00:03<00:05, 41.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 156/395 [00:03<00:05, 40.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 161/395 [00:03<00:05, 41.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 166/395 [00:04<00:05, 41.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 171/395 [00:04<00:05, 42.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 176/395 [00:04<00:05, 42.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 181/395 [00:04<00:04, 43.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 186/395 [00:04<00:04, 42.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 191/395 [00:04<00:04, 43.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 196/395 [00:04<00:04, 43.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 201/395 [00:04<00:04, 43.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 206/395 [00:04<00:04, 44.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 211/395 [00:05<00:04, 42.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 216/395 [00:05<00:04, 42.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 221/395 [00:05<00:04, 42.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 226/395 [00:05<00:03, 42.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 231/395 [00:05<00:03, 42.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 236/395 [00:05<00:03, 42.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 241/395 [00:05<00:03, 42.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 246/395 [00:05<00:03, 42.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 251/395 [00:05<00:03, 43.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 256/395 [00:06<00:03, 42.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 261/395 [00:06<00:03, 43.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 266/395 [00:06<00:02, 43.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 271/395 [00:06<00:02, 43.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 276/395 [00:06<00:02, 43.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 281/395 [00:06<00:02, 44.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 286/395 [00:06<00:02, 39.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 291/395 [00:06<00:02, 37.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 295/395 [00:07<00:02, 35.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 299/395 [00:07<00:02, 34.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 303/395 [00:07<00:02, 35.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 307/395 [00:07<00:02, 35.08it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 311/395 [00:07<00:02, 35.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 315/395 [00:07<00:02, 35.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 319/395 [00:07<00:02, 33.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 323/395 [00:07<00:02, 33.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 327/395 [00:08<00:02, 33.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 331/395 [00:08<00:01, 33.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 335/395 [00:08<00:01, 33.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 339/395 [00:08<00:01, 34.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 343/395 [00:08<00:01, 34.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 347/395 [00:08<00:01, 34.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 351/395 [00:08<00:01, 34.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 356/395 [00:08<00:01, 36.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 361/395 [00:08<00:00, 38.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 365/395 [00:09<00:00, 37.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 369/395 [00:09<00:00, 35.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 373/395 [00:09<00:00, 32.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 377/395 [00:09<00:00, 32.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 381/395 [00:09<00:00, 33.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 385/395 [00:09<00:00, 34.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 389/395 [00:09<00:00, 34.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 394/395 [00:09<00:00, 36.30it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-04-09 13:03:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.844 | nll_loss 15.808 | ppl 57385.8 | wps 2918.6 | wpb 73.1 | bsz 2.5 | num_updates 30\n",
            "2025-04-09 13:03:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30 updates\n",
            "2025-04-09 13:03:20 | INFO | fairseq.trainer | Saving checkpoint to /content/finetuning/dataset/model/checkpoint_best.pt\n",
            "2025-04-09 13:11:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/finetuning/dataset/model/checkpoint_best.pt\n",
            "2025-04-09 13:16:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../dataset/model/checkpoint_best.pt (epoch 1 @ 30 updates, score 15.844) (writing took 775.9668620249995 seconds)\n",
            "2025-04-09 13:16:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2025-04-09 13:16:16 | INFO | train | epoch 001 | loss 16.125 | nll_loss 16.119 | ppl 71189.4 | wps 5.7 | ups 0.04 | wpb 153.3 | bsz 3.9 | num_updates 30 | lr 3.2425e-07 | gnorm 15.047 | clip 100 | loss_scale 128 | train_wall 12 | gb_free 2.9 | wall 803\n",
            "2025-04-09 13:16:16 | INFO | fairseq_cli.train | done training in 798.8 seconds\n"
          ]
        }
      ],
      "source": [
        "# Finetuning the model\n",
        "\n",
        "# pls refer to fairseq documentaion to know more about each of these options (https://fairseq.readthedocs.io/en/latest/command_line_tools.html)\n",
        "\n",
        "\n",
        "# some notable args:\n",
        "# --max-update=1000     -> for this example, to demonstrate how to finetune we are only training for 1000 steps. You should increase this when finetuning\n",
        "# --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
        "# --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
        "# --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
        "# --restore-file        -> reload the pretrained checkpoint and start training from here (change this path for indic-en. Currently its is set to en-indic)\n",
        "# --reset-*             -> reset and not use lr scheduler, dataloader, optimizer etc of the older checkpoint\n",
        "# --max_tokns           -> this is max tokens per batch\n",
        "\n",
        "\n",
        "!( fairseq-train ../dataset/final_bin \\\n",
        "--max-source-positions=210 \\\n",
        "--max-target-positions=210 \\\n",
        "--max-update=500 \\\n",
        "--save-interval=1 \\\n",
        "--arch=transformer_4x \\\n",
        "--criterion=label_smoothed_cross_entropy \\\n",
        "--source-lang=SRC \\\n",
        "--lr-scheduler=inverse_sqrt \\\n",
        "--target-lang=TGT \\\n",
        "--label-smoothing=0.1 \\\n",
        "--optimizer adam \\\n",
        "--adam-betas \"(0.9, 0.98)\" \\\n",
        "--clip-norm 1.0 \\\n",
        "--warmup-init-lr 1e-07 \\\n",
        "--warmup-updates 4000 \\\n",
        "--dropout 0.2 \\\n",
        "--tensorboard-logdir ../dataset/tensorboard-wandb \\\n",
        "--save-dir ../dataset/model \\\n",
        "--keep-last-epochs 5 \\\n",
        "--patience 5 \\\n",
        "--skip-invalid-size-inputs-valid-test \\\n",
        "--fp16 \\\n",
        "--user-dir model_configs \\\n",
        "--update-freq=2 \\\n",
        "--distributed-world-size 1 \\\n",
        "--max-tokens 256 \\\n",
        "--lr 3e-5 \\\n",
        "--restore-file ../m2m/model/checkpoint_best.pt \\\n",
        "--reset-lr-scheduler \\\n",
        "--reset-meters \\\n",
        "--reset-dataloader \\\n",
        "--reset-optimizer)\n",
        "\n",
        "!cp -r /content/finetuning/dataset /content/drive/MyDrive/fairseq_finetuned_dataset/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!exp_dir= /content/finetuning/dataset"
      ],
      "metadata": {
        "id": "vRs0kUgKAmFY",
        "outputId": "5bdf8b79-4257-41b1-c5e4-3b90f69e9e90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: /content/finetuning/dataset: Is a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/finetuning/dataset/test/\n",
        "!ls /content/finetuning/dataset/model/"
      ],
      "metadata": {
        "id": "YWPoucl_B5Ma",
        "outputId": "2eb464a0-db56-4648-a0ad-8c3d1c8638f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cached_lm_test.en  test.en  test.hi  test.kn  test.ml  test.mr\ttest.or  test.pa  test.ta  test.te\n",
            "checkpoint_best.pt  checkpoint_last.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash joint_translate.sh /content/test1.hi /content/sample_data/hi_en_outputs.txt 'hi' 'en' /content/finetuning/dataset"
      ],
      "metadata": {
        "id": "gaCLXqmdCBAJ",
        "outputId": "e08c6841-a853-4b03-dff6-805002e5f0b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 9 01:42:41 PM UTC 2025\n",
            "Applying normalization and script conversion\n",
            "100% 19/19 [00:00<00:00, 296.11it/s]\n",
            "Number of sentences in input: 19\n",
            "Applying BPE\n",
            "Decoding\n",
            "Extracting translations, script conversion and detokenization\n",
            "Translation completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/finetuning/indicTrans"
      ],
      "metadata": {
        "id": "1r4dZXnwC_DB",
        "outputId": "cfeb34a6-8b81-47dd-b935-5bdf42e4aef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning/indicTrans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpPsT1e7vuO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fe68d4-a19d-4733-cf63-121a696ea6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 9 10:06:33 AM UTC 2025\n",
            "Applying normalization and script conversion\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetuning/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
            "    print(preprocess(infname, outfname, lang, transliterate))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/finetuning/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
            "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
            "                                  ^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/test/test.en'\n",
            "Number of sentences in input: \n",
            "Applying BPE\n",
            "joint_translate.sh: line 27: en_hi_outputs.txt.norm: No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetuning/indicTrans/scripts/add_tags_translate.py\", line 28, in <module>\n",
            "    with open(infname, 'r', encoding='utf-8') as infile, \\\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'en_hi_outputs.txt._bpe'\n",
            "Decoding\n",
            "Extracting translations, script conversion and detokenization\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetuning/indicTrans/scripts/postprocess_translate.py\", line 94, in <module>\n",
            "    input_size = int(sys.argv[3])\n",
            "                 ^^^^^^^^^^^^^^^^\n",
            "ValueError: invalid literal for int() with base 10: 'hi'\n",
            "Translation completed\n"
          ]
        }
      ],
      "source": [
        "# To test the models after training, you can use joint_translate.sh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# joint_translate takes src_file, output_fname, src_lang, tgt_lang, model_folder as inputs\n",
        "# src_file -> input text file to be translated\n",
        "# output_fname -> name of the output file (will get created) containing the model predictions\n",
        "# src_lang -> source lang code of the input text ( in this case we are using en-indic model and hence src_lang would be 'en')\n",
        "# tgt_lang -> target lang code of the input text ( tgt lang for en-indic model would be any of the 11 indic langs we trained on:\n",
        "#              as, bn, hi, gu, kn, ml, mr, or, pa, ta, te)\n",
        "# supported languages are:\n",
        "#              as - assamese, bn - bengali, gu - gujarathi, hi - hindi, kn - kannada,\n",
        "#              ml - malayalam, mr - marathi, or - oriya, pa - punjabi, ta - tamil, te - telugu\n",
        "\n",
        "# model_dir -> the directory containing the model and the vocab files\n",
        "\n",
        "# Note: if the translation is taking a lot of time, please tune the buffer_size and batch_size parameter for fairseq-interactive defined inside this joint_translate script\n",
        "\n",
        "\n",
        "# here we are translating the english sentences to hindi\n",
        "!bash joint_translate.sh $exp_dir/test/test.en en_hi_outputs.txt 'en' 'hi' $exp_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPqneByPxilN"
      },
      "outputs": [],
      "source": [
        "# to compute bleu scores for the predicitions with a reference file, use the following command\n",
        "# arguments:\n",
        "# pred_fname: file that contains model predictions\n",
        "# ref_fname: file that contains references\n",
        "# src_lang and tgt_lang : the source and target language\n",
        "\n",
        "bash compute_bleu.sh en_hi_outputs.txt $exp_dir/test/test.hi 'en' 'hi'\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "interpreter": {
      "hash": "3c7d4130300118f0c7487d576c6841c0dbbdeec039e1e658ac9b107412a09af0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}