{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx3z9n0WS4ie",
        "outputId": "3c877bb7-a681-4c02-bd8b-9961be0e1868"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n"
      ],
      "metadata": {
        "id": "Zmy8oookU620"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rE4MO-8bDtwD",
        "outputId": "7c201124-186a-47e5-f418-2313a6c509e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning/finetuning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/finetuning/finetuning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# create a seperate folder to store everything\n",
        "!mkdir finetuning\n",
        "%cd finetuning\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gdown\n",
        "!gdown --id 1-hzy09qi-OEogyge7rQG79K7iV4xsNWa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a--pchvQRnDI",
        "outputId": "eb020a1e-e94a-4f02-b7f2-44123ac9770f",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-hzy09qi-OEogyge7rQG79K7iV4xsNWa\n",
            "From (redirected): https://drive.google.com/uc?id=1-hzy09qi-OEogyge7rQG79K7iV4xsNWa&confirm=t&uuid=ba577d59-e4f7-4544-9933-3e6f0b23668e\n",
            "To: /content/finetuning/finetuning/indic-en.zip\n",
            "100% 4.76G/4.76G [00:57<00:00, 83.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip indic-en.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJO394ngSQd5",
        "outputId": "c1a503e8-0125-4bb8-d593-d6b34202beb8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  indic-en.zip\n",
            "   creating: indic-en/\n",
            "   creating: indic-en/vocab/\n",
            "  inflating: indic-en/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: indic-en/vocab/vocab.SRC  \n",
            "  inflating: indic-en/vocab/vocab.TGT  \n",
            "  inflating: indic-en/vocab/bpe_codes.32k.TGT  \n",
            "   creating: indic-en/final_bin/\n",
            "  inflating: indic-en/final_bin/preprocess.log  \n",
            "  inflating: indic-en/final_bin/dict.TGT.txt  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/dict.SRC.txt  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: indic-en/model/\n",
            "  inflating: indic-en/model/checkpoint_best.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/dataset /content/finetuning"
      ],
      "metadata": {
        "id": "FGi_Xmxsliyp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/finetuning/indic-en/vocab"
      ],
      "metadata": {
        "id": "K_Eh7waqcLfm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/finetuning/dataset"
      ],
      "metadata": {
        "id": "koRh-F4jbKkO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/vocab_bhoj/vocab /content/finetuning/indic-en"
      ],
      "metadata": {
        "id": "-g2sNavsHkVc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2Rs6_WkD_gF",
        "outputId": "acc55353-f577-4b26-8afd-46695738268c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 700, done.\u001b[K\n",
            "remote: Counting objects: 100% (403/403), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 700 (delta 280), reused 343 (delta 240), pack-reused 297 (from 1)\u001b[K\n",
            "Receiving objects: 100% (700/700), 2.64 MiB | 14.44 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "/content/finetuning/finetuning/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1404, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 1404 (delta 139), reused 152 (delta 124), pack-reused 1219 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1404/1404), 9.57 MiB | 10.10 MiB/s, done.\n",
            "Resolving deltas: 100% (749/749), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126 (from 1)\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 40.52 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 622, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 622 (delta 25), reused 31 (delta 16), pack-reused 576 (from 1)\u001b[K\n",
            "Receiving objects: 100% (622/622), 261.27 KiB | 7.26 MiB/s, done.\n",
            "Resolving deltas: 100% (374/374), done.\n",
            "/content/finetuning/finetuning\n",
            "/content/finetuning/finetuning\n"
          ]
        }
      ],
      "source": [
        "# clone the repo for running finetuning\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd ..\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt install tree\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CxSg9C5FV-RW",
        "outputId": "e9aa59f1-87c6-47d3-8b62-a42fd038ed22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 1s (86.8 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting mock\n",
            "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.4)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.4.26)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.92 mock-5.2.0 morfessor-2.0.6 portalocker-3.1.1 sacrebleu-2.5.1 sacremoses-0.1.1 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "duwTvJ9xEBJ1",
        "outputId": "00f90d8a-955f-4854-fd80-9a25fa876164",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq-fixed\n",
            "  Downloading fairseq_fixed-0.12.3.1.tar.gz (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (3.0.12)\n",
            "Collecting hydra-core==1.3.2 (from fairseq-fixed)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting omegaconf==2.3.0 (from fairseq-fixed)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2024.11.6)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2.5.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (4.67.1)\n",
            "Collecting bitarray (from fairseq-fixed)\n",
            "  Downloading bitarray-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (1.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fairseq-fixed) (24.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3.2->fairseq-fixed)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf==2.3.0->fairseq-fixed) (6.0.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq-fixed) (3.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq-fixed) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq-fixed) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.4.12->fairseq-fixed) (5.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->fairseq-fixed)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->fairseq-fixed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->fairseq-fixed) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->fairseq-fixed) (2.22)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fairseq-fixed) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fairseq-fixed) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fairseq-fixed) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->fairseq-fixed) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairseq-fixed, antlr4-python3-runtime\n",
            "  Building wheel for fairseq-fixed (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq-fixed: filename=fairseq_fixed-0.12.3.1-cp311-cp311-linux_x86_64.whl size=20801916 sha256=da66c00226623803bf6fd5a2aa35b2e53e22c32dfa80fe9c5dfab4105e109c5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/c7/c0/a3af3234ddacbe945b1e65290e6474ba35fb0b6e74b84ca1d3\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=09471d10ac430895532b6efa6138b7967a7461063728a2384e638d8c470303b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built fairseq-fixed antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq-fixed\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 bitarray-3.3.2 fairseq-fixed-0.12.3.1 hydra-core-1.3.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "dc0c6d6d91a445d797a78aaef8c1de59"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install fairseq-fixed\n",
        "# Install fairseq from source\n",
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# %cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "# !pip install ./\n",
        "# ! pip install xformers\n",
        "# %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_B4OHdoz_ikR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add fairseq folder to python path\n",
        "# import os\n",
        "# os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ],
      "metadata": {
        "id": "qYNGSXRi1pmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/finetuning/indicTrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWQhCO6UTsbx",
        "outputId": "834a960b-7c6b-4927-8d06-318b4f249f53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/finetuning/indicTrans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhwUXyYVXrOY",
        "outputId": "c8e8e2b9-455b-4089-ed5a-73b0767e5de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment ../dataset on indic to bho\n",
            "Applying normalization and script conversion for train hi\n",
            "100% 15391/15391 [00:00<00:00, 16757.30it/s]\n",
            "100% 15391/15391 [00:00<00:00, 15776.84it/s]\n",
            "Number of sentences in train hi: Warning: 'bho' not supported, defaulting to 'hi' for normalization.\n",
            "15391\n",
            "Applying normalization and script conversion for dev hi\n",
            "100% 810/810 [00:00<00:00, 5958.83it/s]\n",
            "100% 810/810 [00:00<00:00, 10731.36it/s]\n",
            "Number of sentences in dev hi: Warning: 'bho' not supported, defaulting to 'hi' for normalization.\n",
            "810\n",
            "Applying normalization and script conversion for test hi\n",
            "100% 9/9 [00:00<00:00, 225.42it/s]\n",
            "100% 9/9 [00:00<00:00, 221.98it/s]\n",
            "Number of sentences in test hi: Warning: 'bho' not supported, defaulting to 'hi' for normalization.\n",
            "9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# all the data preparation happens in this cell\n",
        "%%shell\n",
        "\n",
        "exp_dir=../dataset\n",
        "src_lang=indic\n",
        "tgt_lang=bho\n",
        "\n",
        "# change this to indic-en, if you have downloaded the indic-en dir or m2m if you have downloaded the indic2indic model\n",
        "download_dir=../indic-en\n",
        "\n",
        "train_data_dir=$exp_dir/train\n",
        "dev_data_dir=$exp_dir/dev\n",
        "test_data_dir=$exp_dir/test\n",
        "\n",
        "\n",
        "echo \"Running experiment ${exp_dir} on ${src_lang} to ${tgt_lang}\"\n",
        "\n",
        "\n",
        "train_processed_dir=$exp_dir/data\n",
        "devtest_processed_dir=$exp_dir/data\n",
        "\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "mkdir -p $train_processed_dir\n",
        "mkdir -p $devtest_processed_dir\n",
        "mkdir -p $out_data_dir\n",
        "\n",
        "# indic languages.\n",
        "# cvit-pib corpus does not have as (assamese) and kn (kannada), hence its not part of this list\n",
        "langs=(hi)\n",
        "\n",
        "for lang in ${langs[@]};do\n",
        "\tif [ $src_lang == en ]; then\n",
        "\t\ttgt_lang=$lang\n",
        "\telse\n",
        "\t\tsrc_lang=$lang\n",
        "\tfi\n",
        "\n",
        "\ttrain_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\tdevtest_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\tmkdir -p $train_norm_dir\n",
        "\tmkdir -p $devtest_norm_dir\n",
        "\n",
        "\n",
        "    # preprocessing pretokenizes the input (we use moses tokenizer for en and indicnlp lib for indic languages)\n",
        "    # after pretokenization, we use indicnlp to transliterate all the indic data to devnagiri script\n",
        "\n",
        "\t# train preprocessing\n",
        "\ttrain_infname_src=$train_data_dir/${lang}-bho/train.$src_lang\n",
        "\ttrain_infname_tgt=$train_data_dir/${lang}-bho/train.$tgt_lang\n",
        "\ttrain_outfname_src=$train_norm_dir/train.$src_lang\n",
        "\ttrain_outfname_tgt=$train_norm_dir/train.$tgt_lang\n",
        "\techo \"Applying normalization and script conversion for train $lang\"\n",
        "\tinput_size=`python scripts/preprocess_translate.py $train_infname_src $train_outfname_src $src_lang true`\n",
        "\tinput_size=`python scripts/preprocess_translate.py $train_infname_tgt $train_outfname_tgt $tgt_lang true`\n",
        "\techo \"Number of sentences in train $lang: $input_size\"\n",
        "\n",
        "\t# dev preprocessing\n",
        "\tdev_infname_src=$dev_data_dir/dev.$src_lang\n",
        "\tdev_infname_tgt=$dev_data_dir/dev.$tgt_lang\n",
        "\tdev_outfname_src=$devtest_norm_dir/dev.$src_lang\n",
        "\tdev_outfname_tgt=$devtest_norm_dir/dev.$tgt_lang\n",
        "\techo \"Applying normalization and script conversion for dev $lang\"\n",
        "\tinput_size=`python scripts/preprocess_translate.py $dev_infname_src $dev_outfname_src $src_lang true`\n",
        "\tinput_size=`python scripts/preprocess_translate.py $dev_infname_tgt $dev_outfname_tgt $tgt_lang true`\n",
        "\techo \"Number of sentences in dev $lang: $input_size\"\n",
        "\n",
        "\t# test preprocessing\n",
        "\ttest_infname_src=$test_data_dir/test.$src_lang\n",
        "\ttest_infname_tgt=$test_data_dir/test.$tgt_lang\n",
        "\ttest_outfname_src=$devtest_norm_dir/test.$src_lang\n",
        "\ttest_outfname_tgt=$devtest_norm_dir/test.$tgt_lang\n",
        "\techo \"Applying normalization and script conversion for test $lang\"\n",
        "\tinput_size=`python scripts/preprocess_translate.py $test_infname_src $test_outfname_src $src_lang true`\n",
        "\tinput_size=`python scripts/preprocess_translate.py $test_infname_tgt $test_outfname_tgt $tgt_lang true`\n",
        "\techo \"Number of sentences in test $lang: $input_size\"\n",
        "done\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have preprocessed all the data, we can now merge these different text files into one\n",
        "# ie. for en-as, we have train.en and corresponding train.as, similarly for en-bn, we have train.en and corresponding train.bn\n",
        "# now we will concatenate all this into en-X where train.SRC will have all the en (src) training data and train.TGT will have all the concatenated indic lang data\n",
        "# all the data preparation happens in this cell\n",
        "%%shell\n",
        "\n",
        "exp_dir=../dataset\n",
        "src_lang=indic\n",
        "tgt_lang=bho\n",
        "\n",
        "# change this to indic-en, if you have downloaded the indic-en dir or m2m if you have downloaded the indic2indic model\n",
        "download_dir=../indic-en\n",
        "\n",
        "train_data_dir=$exp_dir/train\n",
        "dev_data_dir=$exp_dir/dev\n",
        "test_data_dir=$exp_dir/test\n",
        "\n",
        "\n",
        "echo \"Running experiment ${exp_dir} on ${src_lang} to ${tgt_lang}\"\n",
        "\n",
        "\n",
        "train_processed_dir=$exp_dir/data\n",
        "devtest_processed_dir=$exp_dir/data\n",
        "\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "\n",
        "\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'train'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'dev'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data $src_lang $tgt_lang 'test'\n",
        "\n",
        "# use the vocab from downloaded dir\n",
        "cp -r $download_dir/vocab $exp_dir\n",
        "\n",
        "\n",
        "echo \"Applying bpe to the new finetuning data\"\n",
        "bash apply_single_bpe_traindevtest_notag.sh $exp_dir\n",
        "\n",
        "mkdir -p $exp_dir/final\n",
        "\n",
        "# We also add special tags to indicate the source and target language in the inputs\n",
        "#  Eg: to translate a sentence from english to hindi , the input would be   __src__en__   __tgt__hi__ <en bpe tokens>\n",
        "\n",
        "echo \"Adding language tags\"\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'train'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'dev'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'test'\n",
        "\n",
        "\n",
        "\n",
        "data_dir=$exp_dir/final\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "rm -rf $out_data_dir\n",
        "\n",
        "# binarizing the new data (train, dev and test) using dictionary from the download dir\n",
        "\n",
        " num_workers=`python -c \"import multiprocessing; print(multiprocessing.cpu_count())\"`\n",
        "\n",
        "data_dir=$exp_dir/final\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "# rm -rf $out_data_dir\n",
        "\n",
        "# echo \"Binarizing data. This will take some time depending on the size of finetuning data\"\n",
        "fairseq-preprocess --source-lang SRC --target-lang TGT \\\n",
        " --trainpref $data_dir/train --validpref $data_dir/dev --testpref $data_dir/test \\\n",
        " --destdir $out_data_dir --workers $num_workers \\\n",
        " --srcdict $download_dir/final_bin/dict.SRC.txt --tgtdict $download_dir/final_bin/dict.TGT.txt --thresholdtgt 5 --thresholdsrc 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeZBASHGbpvr",
        "outputId": "e9f2f9cb-207f-4c4e-bdbd-0d03b66d8f2c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment ../dataset on indic to bho\n",
            "\n",
            "../dataset/data/train.SRC\n",
            "../dataset/data/train.TGT\n",
            "  0% 0/12 [00:00<?, ?it/s]src: as, tgt:bho\n",
            "src: bn, tgt:bho\n",
            "src: gu, tgt:bho\n",
            "src: hi, tgt:bho\n",
            "../dataset/norm/hi-bho/train.hi\n",
            "../dataset/norm/hi-bho/train.bho\n",
            "src: kn, tgt:bho\n",
            "src: ml, tgt:bho\n",
            "src: mr, tgt:bho\n",
            "src: or, tgt:bho\n",
            "src: pa, tgt:bho\n",
            "src: ta, tgt:bho\n",
            "src: te, tgt:bho\n",
            "src: bho, tgt:bho\n",
            "100% 12/12 [00:00<00:00, 956.06it/s]\n",
            "  0% 0/12 [00:00<?, ?it/s]src: as, tgt:bho\n",
            "src: bn, tgt:bho\n",
            "src: gu, tgt:bho\n",
            "src: hi, tgt:bho\n",
            "../dataset/norm/hi-bho/train.hi\n",
            "src: kn, tgt:bho\n",
            "src: ml, tgt:bho\n",
            "src: mr, tgt:bho\n",
            "src: or, tgt:bho\n",
            "src: pa, tgt:bho\n",
            "src: ta, tgt:bho\n",
            "src: te, tgt:bho\n",
            "src: bho, tgt:bho\n",
            "100% 12/12 [00:00<00:00, 689.84it/s]\n",
            "\n",
            "../dataset/data/dev.SRC\n",
            "../dataset/data/dev.TGT\n",
            "  0% 0/12 [00:00<?, ?it/s]src: as, tgt:bho\n",
            "src: bn, tgt:bho\n",
            "src: gu, tgt:bho\n",
            "src: hi, tgt:bho\n",
            "../dataset/norm/hi-bho/dev.hi\n",
            "../dataset/norm/hi-bho/dev.bho\n",
            "src: kn, tgt:bho\n",
            "src: ml, tgt:bho\n",
            "src: mr, tgt:bho\n",
            "src: or, tgt:bho\n",
            "src: pa, tgt:bho\n",
            "src: ta, tgt:bho\n",
            "src: te, tgt:bho\n",
            "src: bho, tgt:bho\n",
            "100% 12/12 [00:00<00:00, 2609.21it/s]\n",
            "  0% 0/12 [00:00<?, ?it/s]src: as, tgt:bho\n",
            "src: bn, tgt:bho\n",
            "src: gu, tgt:bho\n",
            "src: hi, tgt:bho\n",
            "../dataset/norm/hi-bho/dev.hi\n",
            "src: kn, tgt:bho\n",
            "src: ml, tgt:bho\n",
            "src: mr, tgt:bho\n",
            "src: or, tgt:bho\n",
            "src: pa, tgt:bho\n",
            "src: ta, tgt:bho\n",
            "src: te, tgt:bho\n",
            "src: bho, tgt:bho\n",
            "100% 12/12 [00:00<00:00, 9803.59it/s]\n",
            "\n",
            "../dataset/data/test.SRC\n",
            "../dataset/data/test.TGT\n",
            "  0% 0/12 [00:00<?, ?it/s]src: as, tgt:bho\n",
            "src: bn, tgt:bho\n",
            "src: gu, tgt:bho\n",
            "src: hi, tgt:bho\n",
            "../dataset/norm/hi-bho/test.hi\n",
            "../dataset/norm/hi-bho/test.bho\n",
            "src: kn, tgt:bho\n",
            "src: ml, tgt:bho\n",
            "src: mr, tgt:bho\n",
            "src: or, tgt:bho\n",
            "src: pa, tgt:bho\n",
            "src: ta, tgt:bho\n",
            "src: te, tgt:bho\n",
            "src: bho, tgt:bho\n",
            "100% 12/12 [00:00<00:00, 3655.70it/s]\n",
            "  0% 0/12 [00:00<?, ?it/s]src: as, tgt:bho\n",
            "src: bn, tgt:bho\n",
            "src: gu, tgt:bho\n",
            "src: hi, tgt:bho\n",
            "../dataset/norm/hi-bho/test.hi\n",
            "src: kn, tgt:bho\n",
            "src: ml, tgt:bho\n",
            "src: mr, tgt:bho\n",
            "src: or, tgt:bho\n",
            "src: pa, tgt:bho\n",
            "src: ta, tgt:bho\n",
            "src: te, tgt:bho\n",
            "src: bho, tgt:bho\n",
            "100% 12/12 [00:00<00:00, 46820.14it/s]\n",
            "Applying bpe to the new finetuning data\n",
            "train\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "dev\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "test\n",
            "Apply to SRC corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Apply to TGT corpus\n",
            "/content/finetuning/indicTrans/subword-nmt/subword_nmt/apply_bpe.py:509: UserWarning: In parallel mode, the input cannot be STDIN. Using 1 processor instead.\n",
            "  warnings.warn(\"In parallel mode, the input cannot be STDIN. Using 1 processor instead.\")\n",
            "Adding language tags\n",
            "15391it [00:00, 215767.95it/s]\n",
            "810it [00:00, 198584.65it/s]\n",
            "9it [00:00, 45535.27it/s]\n",
            "2025-05-03 08:57:15.684695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746262635.722978   14953 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746262635.736805   14953 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-03 08:57:15.781716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-03 08:57:25 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='SRC', target_lang='TGT', trainpref='../dataset/final/train', validpref='../dataset/final/dev', testpref='../dataset/final/test', align_suffix=None, destdir='../dataset/final_bin', thresholdtgt=5, thresholdsrc=5, tgtdict='../indic-en/final_bin/dict.TGT.txt', srcdict='../indic-en/final_bin/dict.SRC.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=2, dict_only=False)\n",
            "2025-05-03 08:57:25 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 35904 types\n",
            "2025-05-03 08:57:29 | INFO | fairseq_cli.preprocess | [SRC] ../dataset/final/train.SRC: 15391 sents, 371380 tokens, 4.17% replaced (by <unk>)\n",
            "2025-05-03 08:57:29 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 35904 types\n",
            "2025-05-03 08:57:30 | INFO | fairseq_cli.preprocess | [SRC] ../dataset/final/dev.SRC: 810 sents, 20307 tokens, 4.0% replaced (by <unk>)\n",
            "2025-05-03 08:57:30 | INFO | fairseq_cli.preprocess | [SRC] Dictionary: 35904 types\n",
            "2025-05-03 08:57:30 | INFO | fairseq_cli.preprocess | [SRC] ../dataset/final/test.SRC: 9 sents, 220 tokens, 4.09% replaced (by <unk>)\n",
            "2025-05-03 08:57:30 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 32088 types\n",
            "2025-05-03 08:57:33 | INFO | fairseq_cli.preprocess | [TGT] ../dataset/final/train.TGT: 15391 sents, 317866 tokens, 0.00849% replaced (by <unk>)\n",
            "2025-05-03 08:57:33 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 32088 types\n",
            "2025-05-03 08:57:34 | INFO | fairseq_cli.preprocess | [TGT] ../dataset/final/dev.TGT: 810 sents, 17470 tokens, 0.0343% replaced (by <unk>)\n",
            "2025-05-03 08:57:34 | INFO | fairseq_cli.preprocess | [TGT] Dictionary: 32088 types\n",
            "2025-05-03 08:57:34 | INFO | fairseq_cli.preprocess | [TGT] ../dataset/final/test.TGT: 9 sents, 183 tokens, 0.0% replaced (by <unk>)\n",
            "2025-05-03 08:57:34 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../dataset/final_bin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LW592WMLd9jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz6tzbe2tcs7",
        "outputId": "72ba232e-eb41-4885-8889-a583d56d81f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-03 08:57:38.245913: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746262658.265618   15159 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746262658.271807   15159 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-03 08:57:38.291895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-03 08:57:41 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  deprecation_warning(message=message)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  self.delegate = real_initialize(\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
            "  deprecation_warning(message=message)\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  deprecation_warning(\n",
            "sys:1: UserWarning: \n",
            "'config' is validated against ConfigStore schema with the same name.\n",
            "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
            "/usr/local/lib/python3.11/dist-packages/hydra/compose.py:56: UserWarning: \n",
            "The strict flag in the compose API is deprecated.\n",
            "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
            "\n",
            "  deprecation_warning(\n",
            "2025-05-03 08:57:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../dataset/tensorboard-wandb', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': 'model_configs', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 256, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 256, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 7000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '../dataset/model', 'restore_file': '../indic-en/model/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 3, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='../dataset/tensorboard-wandb', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir='model_configs', empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=256, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=256, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_4x', max_epoch=0, max_update=7000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='../dataset/model', restore_file='../indic-en/model/checkpoint_best.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=3, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='../dataset/final_bin', source_lang='SRC', target_lang='TGT', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, max_source_positions=210, max_target_positions=210, dropout=0.2, no_seed_provided=False, encoder_embed_dim=1536, encoder_ffn_embed_dim=4096, encoder_attention_heads=16, encoder_normalize_before=False, decoder_embed_dim=1536, decoder_ffn_embed_dim=4096, decoder_attention_heads=16, encoder_embed_path=None, encoder_layers=6, encoder_learned_pos=False, decoder_embed_path=None, decoder_layers=6, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1536, decoder_input_dim=1536, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_4x'), 'task': {'_name': 'translation', 'data': '../dataset/final_bin', 'source_lang': 'SRC', 'target_lang': 'TGT', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 210, 'max_target_positions': 210, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2025-05-03 08:57:46 | INFO | fairseq.tasks.translation | [SRC] dictionary: 35904 types\n",
            "2025-05-03 08:57:46 | INFO | fairseq.tasks.translation | [TGT] dictionary: 32088 types\n",
            "2025-05-03 08:57:51 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(35904, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(32088, 1536, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1536, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=1536, out_features=32088, bias=False)\n",
            "  )\n",
            ")\n",
            "2025-05-03 08:57:51 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2025-05-03 08:57:51 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2025-05-03 08:57:51 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2025-05-03 08:57:51 | INFO | fairseq_cli.train | num. shared model params: 474,857,472 (num. trained: 474,857,472)\n",
            "2025-05-03 08:57:51 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2025-05-03 08:57:51 | INFO | fairseq.data.data_utils | loaded 810 examples from: ../dataset/final_bin/valid.SRC-TGT.SRC\n",
            "2025-05-03 08:57:51 | INFO | fairseq.data.data_utils | loaded 810 examples from: ../dataset/final_bin/valid.SRC-TGT.TGT\n",
            "2025-05-03 08:57:51 | INFO | fairseq.tasks.translation | ../dataset/final_bin valid SRC-TGT 810 examples\n",
            "2025-05-03 08:57:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2025-05-03 08:57:53 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.741 GB ; name = Tesla T4                                \n",
            "2025-05-03 08:57:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2025-05-03 08:57:53 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2025-05-03 08:57:53 | INFO | fairseq_cli.train | max tokens per device = 256 and max sentences per device = None\n",
            "2025-05-03 08:57:53 | INFO | fairseq.trainer | Preparing to load checkpoint ../indic-en/model/checkpoint_best.pt\n",
            "2025-05-03 08:58:21 | INFO | fairseq.trainer | Loaded checkpoint ../indic-en/model/checkpoint_best.pt (epoch 3 @ 0 updates)\n",
            "2025-05-03 08:58:21 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2025-05-03 08:58:21 | INFO | fairseq.data.data_utils | loaded 15,391 examples from: ../dataset/final_bin/train.SRC-TGT.SRC\n",
            "2025-05-03 08:58:21 | INFO | fairseq.data.data_utils | loaded 15,391 examples from: ../dataset/final_bin/train.SRC-TGT.TGT\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.translation | ../dataset/final_bin train SRC-TGT 15391 examples\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2025-05-03 08:58:21 | WARNING | fairseq.tasks.fairseq_task | 5 samples have invalid sizes and will be skipped, max_positions=(210, 210), first few sample ids=[4534, 13455, 2712, 8976, 4393]\n",
            "2025-05-03 08:58:21 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2025-05-03 08:58:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2025-05-03 08:58:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 940\n",
            "epoch 001:   0% 0/940 [00:00<?, ?it/s]2025-05-03 08:58:22 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2025-05-03 08:58:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.11/dist-packages/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n",
            "2025-05-03 08:58:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   0% 1/940 [00:01<24:28,  1.56s/it]2025-05-03 08:58:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:   0% 3/940 [00:02<09:55,  1.57it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 001:   1% 7/940 [00:03<05:34,  2.79it/s]2025-05-03 08:58:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001: 100% 939/940 [04:52<00:00,  3.24it/s, loss=10.889, nll_loss=10.225, ppl=1197.08, wps=1052.1, ups=3.21, wpb=327.6, bsz=15.9, num_updates=900, lr=6.8275e-06, gnorm=4.638, clip=100, loss_scale=16, train_wall=31, gb_free=4.1, wall=310]2025-05-03 09:03:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-05-03 09:03:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 3/112 [00:00<00:04, 21.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 8/112 [00:00<00:03, 33.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 13/112 [00:00<00:02, 38.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 18/112 [00:00<00:02, 40.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 23/112 [00:00<00:02, 42.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 28/112 [00:00<00:01, 43.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 33/112 [00:00<00:01, 44.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 38/112 [00:00<00:01, 45.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 43/112 [00:01<00:01, 45.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 48/112 [00:01<00:01, 46.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 53/112 [00:01<00:01, 43.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 58/112 [00:01<00:01, 44.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 63/112 [00:01<00:01, 44.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 68/112 [00:01<00:00, 44.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 73/112 [00:01<00:00, 44.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 78/112 [00:01<00:00, 45.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 83/112 [00:01<00:00, 44.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 88/112 [00:02<00:00, 44.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 93/112 [00:02<00:00, 44.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 98/112 [00:02<00:00, 42.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 104/112 [00:02<00:00, 45.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 110/112 [00:02<00:00, 48.56it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-05-03 09:03:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.804 | nll_loss 10.122 | ppl 1114.65 | wps 7000.6 | wpb 154.5 | bsz 7.2 | num_updates 937\n",
            "2025-05-03 09:03:17 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2025-05-03 09:03:17 | INFO | train | epoch 001 | loss 11.784 | nll_loss 11.227 | ppl 2396.45 | wps 1077.6 | ups 3.2 | wpb 336.9 | bsz 16.4 | num_updates 937 | lr 7.10408e-06 | gnorm 6.973 | clip 100 | loss_scale 16 | train_wall 287 | gb_free 4.1 | wall 324\n",
            "2025-05-03 09:03:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 09:03:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 940\n",
            "epoch 002:   0% 0/940 [00:00<?, ?it/s]2025-05-03 09:03:17 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2025-05-03 09:03:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 939/940 [04:51<00:00,  3.34it/s, loss=10.165, nll_loss=9.367, ppl=660.22, wps=1091, ups=3.24, wpb=336.3, bsz=16.2, num_updates=1800, lr=1.3555e-05, gnorm=5.117, clip=100, loss_scale=16, train_wall=30, gb_free=4.1, wall=592]2025-05-03 09:08:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-05-03 09:08:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 2/112 [00:00<00:05, 18.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 7/112 [00:00<00:03, 32.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 12/112 [00:00<00:02, 36.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 17/112 [00:00<00:02, 40.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 22/112 [00:00<00:02, 41.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 27/112 [00:00<00:02, 42.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 32/112 [00:00<00:01, 43.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 37/112 [00:00<00:01, 43.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 42/112 [00:01<00:01, 44.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 47/112 [00:01<00:01, 43.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 52/112 [00:01<00:01, 43.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 57/112 [00:01<00:01, 44.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 62/112 [00:01<00:01, 44.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 67/112 [00:01<00:01, 44.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 72/112 [00:01<00:00, 44.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 77/112 [00:01<00:00, 44.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 82/112 [00:01<00:00, 43.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 87/112 [00:02<00:00, 44.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 92/112 [00:02<00:00, 43.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 97/112 [00:02<00:00, 43.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 102/112 [00:02<00:00, 45.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 108/112 [00:02<00:00, 48.29it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-05-03 09:08:11 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.939 | nll_loss 9.106 | ppl 551.17 | wps 6953.4 | wpb 154.5 | bsz 7.2 | num_updates 1877\n",
            "2025-05-03 09:08:11 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2025-05-03 09:08:11 | INFO | train | epoch 002 | loss 10.408 | nll_loss 9.65 | ppl 803.65 | wps 1077.8 | ups 3.2 | wpb 337 | bsz 16.4 | num_updates 1877 | lr 1.41306e-05 | gnorm 4.74 | clip 100 | loss_scale 16 | train_wall 286 | gb_free 4.1 | wall 618\n",
            "2025-05-03 09:08:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 09:08:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 940\n",
            "epoch 003:   0% 0/940 [00:00<?, ?it/s]2025-05-03 09:08:11 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2025-05-03 09:08:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 939/940 [04:50<00:00,  3.41it/s, loss=9.111, nll_loss=8.174, ppl=288.91, wps=1087.6, ups=3.21, wpb=339.1, bsz=15.4, num_updates=2800, lr=2.103e-05, gnorm=6.461, clip=100, loss_scale=16, train_wall=31, gb_free=4.1, wall=904]2025-05-03 09:13:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-05-03 09:13:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 2/112 [00:00<00:05, 18.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   6% 7/112 [00:00<00:03, 33.77it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 12/112 [00:00<00:02, 38.89it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 17/112 [00:00<00:02, 41.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 22/112 [00:00<00:02, 42.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 27/112 [00:00<00:01, 44.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 32/112 [00:00<00:01, 44.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 37/112 [00:00<00:01, 45.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 42/112 [00:00<00:01, 45.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 47/112 [00:01<00:01, 46.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  46% 52/112 [00:01<00:01, 44.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  51% 57/112 [00:01<00:01, 46.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 62/112 [00:01<00:01, 45.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 67/112 [00:01<00:01, 44.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 72/112 [00:01<00:00, 44.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  69% 77/112 [00:01<00:00, 44.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 82/112 [00:01<00:00, 44.57it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  78% 87/112 [00:01<00:00, 44.67it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 92/112 [00:02<00:00, 45.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  87% 97/112 [00:02<00:00, 43.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 102/112 [00:02<00:00, 44.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 108/112 [00:02<00:00, 47.73it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-05-03 09:13:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.968 | nll_loss 8 | ppl 256.02 | wps 7080.5 | wpb 154.5 | bsz 7.2 | num_updates 2817\n",
            "2025-05-03 09:13:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2817 updates\n",
            "2025-05-03 09:13:04 | INFO | fairseq.trainer | Saving checkpoint to /content/finetuning/dataset/model/checkpoint3.pt\n",
            "2025-05-03 09:17:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/finetuning/dataset/model/checkpoint3.pt\n",
            "2025-05-03 09:24:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../dataset/model/checkpoint3.pt (epoch 3 @ 2817 updates, score 8.968) (writing took 668.9919798560004 seconds)\n",
            "2025-05-03 09:24:13 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2025-05-03 09:24:13 | INFO | train | epoch 003 | loss 9.417 | nll_loss 8.517 | ppl 366.27 | wps 329 | ups 0.98 | wpb 337 | bsz 16.4 | num_updates 2817 | lr 2.11571e-05 | gnorm 6.071 | clip 100 | loss_scale 16 | train_wall 286 | gb_free 4.1 | wall 1581\n",
            "2025-05-03 09:24:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 09:24:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 940\n",
            "epoch 004:   0% 0/940 [00:00<?, ?it/s]2025-05-03 09:24:13 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2025-05-03 09:24:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 939/940 [04:47<00:00,  3.34it/s, loss=7.982, nll_loss=6.889, ppl=118.55, wps=1144.8, ups=3.3, wpb=346.8, bsz=16.6, num_updates=3700, lr=2.77575e-05, gnorm=7.065, clip=100, loss_scale=16, train_wall=30, gb_free=4.1, wall=1852]2025-05-03 09:29:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-05-03 09:29:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 2/112 [00:00<00:07, 15.04it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 5/112 [00:00<00:04, 21.42it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 9/112 [00:00<00:03, 27.03it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 13/112 [00:00<00:03, 28.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 17/112 [00:00<00:03, 29.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  19% 21/112 [00:00<00:02, 32.07it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  22% 25/112 [00:00<00:02, 34.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 29/112 [00:00<00:02, 35.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  29% 33/112 [00:01<00:02, 36.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 37/112 [00:01<00:02, 36.46it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  37% 41/112 [00:01<00:02, 35.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  40% 45/112 [00:01<00:01, 35.66it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 49/112 [00:01<00:01, 36.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 53/112 [00:01<00:01, 35.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  51% 57/112 [00:01<00:01, 35.69it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  54% 61/112 [00:01<00:01, 33.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 65/112 [00:01<00:01, 33.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 69/112 [00:02<00:01, 33.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 73/112 [00:02<00:01, 33.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  70% 78/112 [00:02<00:00, 35.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  74% 83/112 [00:02<00:00, 37.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 88/112 [00:02<00:00, 40.39it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 93/112 [00:02<00:00, 42.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 98/112 [00:02<00:00, 42.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  93% 104/112 [00:02<00:00, 46.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98% 110/112 [00:02<00:00, 49.06it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-05-03 09:29:04 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.871 | nll_loss 6.742 | ppl 107.05 | wps 5857.5 | wpb 154.5 | bsz 7.2 | num_updates 3757 | best_loss 7.871\n",
            "2025-05-03 09:29:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2025-05-03 09:29:04 | INFO | train | epoch 004 | loss 8.326 | nll_loss 7.278 | ppl 155.16 | wps 1088.5 | ups 3.23 | wpb 337 | bsz 16.4 | num_updates 3757 | lr 2.81836e-05 | gnorm 6.98 | clip 100 | loss_scale 16 | train_wall 283 | gb_free 4.1 | wall 1872\n",
            "2025-05-03 09:29:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 09:29:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 940\n",
            "epoch 005:   0% 0/940 [00:00<?, ?it/s]2025-05-03 09:29:04 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2025-05-03 09:29:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 939/940 [04:45<00:00,  3.01it/s, loss=6.984, nll_loss=5.751, ppl=53.87, wps=1106.9, ups=3.3, wpb=335.6, bsz=16.9, num_updates=4600, lr=2.79751e-05, gnorm=7.7, clip=100, loss_scale=16, train_wall=30, gb_free=4.1, wall=2128]2025-05-03 09:33:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-05-03 09:33:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   2% 2/112 [00:00<00:05, 19.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 7/112 [00:00<00:03, 33.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 12/112 [00:00<00:02, 38.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 17/112 [00:00<00:02, 42.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 22/112 [00:00<00:02, 42.71it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 27/112 [00:00<00:01, 42.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 32/112 [00:00<00:01, 44.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 37/112 [00:00<00:01, 45.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 42/112 [00:00<00:01, 45.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 47/112 [00:01<00:01, 45.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  46% 52/112 [00:01<00:01, 46.42it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  51% 57/112 [00:01<00:01, 46.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 62/112 [00:01<00:01, 46.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  60% 67/112 [00:01<00:00, 46.28it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 72/112 [00:01<00:00, 46.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  69% 77/112 [00:01<00:00, 44.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 82/112 [00:01<00:00, 44.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 87/112 [00:01<00:00, 44.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 92/112 [00:02<00:00, 44.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  87% 97/112 [00:02<00:00, 44.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 103/112 [00:02<00:00, 46.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 108/112 [00:02<00:00, 44.81it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-05-03 09:33:53 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.075 | nll_loss 5.812 | ppl 56.17 | wps 7028.5 | wpb 154.5 | bsz 7.2 | num_updates 4697 | best_loss 7.075\n",
            "2025-05-03 09:33:53 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2025-05-03 09:33:53 | INFO | train | epoch 005 | loss 7.245 | nll_loss 6.047 | ppl 66.11 | wps 1097.1 | ups 3.26 | wpb 337 | bsz 16.4 | num_updates 4697 | lr 2.76848e-05 | gnorm 7.558 | clip 100 | loss_scale 16 | train_wall 281 | gb_free 4.1 | wall 2160\n",
            "2025-05-03 09:33:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 09:33:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 940\n",
            "epoch 006:   0% 0/940 [00:00<?, ?it/s]2025-05-03 09:33:53 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2025-05-03 09:33:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100% 939/940 [04:47<00:00,  3.37it/s, loss=6.215, nll_loss=4.874, ppl=29.33, wps=1089.5, ups=3.29, wpb=331.4, bsz=17.1, num_updates=5600, lr=2.53546e-05, gnorm=7.839, clip=100, loss_scale=16, train_wall=30, gb_free=4.1, wall=2437]2025-05-03 09:38:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-05-03 09:38:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   3% 3/112 [00:00<00:04, 25.28it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   7% 8/112 [00:00<00:02, 38.14it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 13/112 [00:00<00:02, 42.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  16% 18/112 [00:00<00:02, 43.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  21% 23/112 [00:00<00:01, 45.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 28/112 [00:00<00:01, 46.13it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 33/112 [00:00<00:01, 47.25it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  34% 38/112 [00:00<00:01, 46.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 43/112 [00:00<00:01, 44.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  43% 48/112 [00:01<00:01, 43.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47% 53/112 [00:01<00:01, 44.27it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 58/112 [00:01<00:01, 44.00it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 63/112 [00:01<00:01, 44.66it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 68/112 [00:01<00:00, 45.16it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 73/112 [00:01<00:00, 45.43it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  70% 78/112 [00:01<00:00, 45.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 83/112 [00:01<00:00, 45.41it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 88/112 [00:01<00:00, 45.71it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 93/112 [00:02<00:00, 44.03it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 98/112 [00:02<00:00, 43.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  93% 104/112 [00:02<00:00, 46.69it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 110/112 [00:02<00:00, 48.99it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-05-03 09:38:43 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.586 | nll_loss 5.217 | ppl 37.19 | wps 7199.8 | wpb 154.5 | bsz 7.2 | num_updates 5637 | best_loss 6.586\n",
            "2025-05-03 09:38:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5637 updates\n",
            "2025-05-03 09:38:43 | INFO | fairseq.trainer | Saving checkpoint to /content/finetuning/dataset/model/checkpoint6.pt\n",
            "2025-05-03 09:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/finetuning/dataset/model/checkpoint6.pt\n",
            "2025-05-03 09:50:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../dataset/model/checkpoint6.pt (epoch 6 @ 5637 updates, score 6.586) (writing took 688.2959403000004 seconds)\n",
            "2025-05-03 09:50:11 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2025-05-03 09:50:11 | INFO | train | epoch 006 | loss 6.335 | nll_loss 5.009 | ppl 32.19 | wps 323.8 | ups 0.96 | wpb 337 | bsz 16.4 | num_updates 5637 | lr 2.52713e-05 | gnorm 7.942 | clip 100 | loss_scale 16 | train_wall 282 | gb_free 4.1 | wall 3139\n",
            "2025-05-03 09:50:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 09:50:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 940\n",
            "epoch 007:   0% 0/940 [00:00<?, ?it/s]2025-05-03 09:50:11 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2025-05-03 09:50:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100% 939/940 [04:49<00:00,  3.35it/s, loss=5.465, nll_loss=4.017, ppl=16.19, wps=1111.6, ups=3.26, wpb=341.1, bsz=16.1, num_updates=6500, lr=2.35339e-05, gnorm=7.69, clip=100, loss_scale=16, train_wall=30, gb_free=4.1, wall=3405]2025-05-03 09:55:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-05-03 09:55:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   3% 3/112 [00:00<00:04, 24.74it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   7% 8/112 [00:00<00:02, 37.47it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 13/112 [00:00<00:02, 40.54it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  16% 18/112 [00:00<00:02, 42.85it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  21% 23/112 [00:00<00:02, 41.80it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 28/112 [00:00<00:01, 44.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  29% 33/112 [00:00<00:01, 45.50it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  34% 38/112 [00:00<00:01, 46.24it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 43/112 [00:00<00:01, 46.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  43% 48/112 [00:01<00:01, 47.07it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  47% 53/112 [00:01<00:01, 45.87it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 58/112 [00:01<00:01, 46.44it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 63/112 [00:01<00:01, 45.87it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 68/112 [00:01<00:01, 43.49it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  65% 73/112 [00:01<00:00, 44.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  70% 78/112 [00:01<00:00, 44.28it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  74% 83/112 [00:01<00:00, 44.72it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  79% 88/112 [00:01<00:00, 44.23it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 93/112 [00:02<00:00, 44.65it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 98/112 [00:02<00:00, 44.20it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  93% 104/112 [00:02<00:00, 47.14it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  97% 109/112 [00:02<00:00, 47.62it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-05-03 09:55:04 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.189 | nll_loss 4.765 | ppl 27.19 | wps 7107.8 | wpb 154.5 | bsz 7.2 | num_updates 6577 | best_loss 6.189\n",
            "2025-05-03 09:55:04 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2025-05-03 09:55:04 | INFO | train | epoch 007 | loss 5.65 | nll_loss 4.228 | ppl 18.74 | wps 1083.7 | ups 3.22 | wpb 337 | bsz 16.4 | num_updates 6577 | lr 2.33958e-05 | gnorm 7.991 | clip 100 | loss_scale 16 | train_wall 285 | gb_free 4.1 | wall 3431\n",
            "2025-05-03 09:55:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2025-05-03 09:55:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 940\n",
            "epoch 008:   0% 0/940 [00:00<?, ?it/s]2025-05-03 09:55:04 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2025-05-03 09:55:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  45% 422/940 [02:09<02:35,  3.33it/s, loss=5.161, nll_loss=3.669, ppl=12.72, wps=1090.7, ups=3.22, wpb=338.6, bsz=16.8, num_updates=6900, lr=2.28416e-05, gnorm=7.912, clip=100, loss_scale=16, train_wall=30, gb_free=4.1, wall=3531]2025-05-03 09:57:14 | INFO | fairseq_cli.train | Stopping training due to num_updates: 7000 >= max_update: 7000\n",
            "2025-05-03 09:57:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2025-05-03 09:57:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/112 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   3% 3/112 [00:00<00:04, 24.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   7% 8/112 [00:00<00:03, 34.25it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 12/112 [00:00<00:03, 33.21it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  14% 16/112 [00:00<00:03, 31.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  18% 20/112 [00:00<00:03, 29.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  22% 25/112 [00:00<00:02, 33.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 30/112 [00:00<00:02, 38.23it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  31% 35/112 [00:00<00:01, 40.90it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 40/112 [00:01<00:01, 43.10it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  40% 45/112 [00:01<00:01, 44.63it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 50/112 [00:01<00:01, 44.79it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  49% 55/112 [00:01<00:01, 44.79it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  54% 60/112 [00:01<00:01, 45.02it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  58% 65/112 [00:01<00:01, 45.02it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 70/112 [00:01<00:00, 45.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 75/112 [00:01<00:00, 45.24it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 80/112 [00:01<00:00, 45.38it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 85/112 [00:02<00:00, 45.68it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  80% 90/112 [00:02<00:00, 45.77it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 95/112 [00:02<00:00, 45.81it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 100/112 [00:02<00:00, 44.86it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  95% 106/112 [00:02<00:00, 47.27it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 112/112 [00:02<00:00, 50.03it/s]\u001b[A\n",
            "                                                                          \u001b[A2025-05-03 09:57:17 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.104 | nll_loss 4.657 | ppl 25.22 | wps 6724.6 | wpb 154.5 | bsz 7.2 | num_updates 7000 | best_loss 6.104\n",
            "2025-05-03 09:57:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 7000 updates\n",
            "2025-05-03 09:57:17 | INFO | fairseq.trainer | Saving checkpoint to /content/finetuning/dataset/model/checkpoint_best.pt\n",
            "2025-05-03 10:04:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/finetuning/dataset/model/checkpoint_best.pt\n",
            "2025-05-03 10:08:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../dataset/model/checkpoint_best.pt (epoch 8 @ 7000 updates, score 6.104) (writing took 661.3462142880007 seconds)\n",
            "2025-05-03 10:08:18 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2025-05-03 10:08:18 | INFO | train | epoch 008 | loss 5.151 | nll_loss 3.659 | ppl 12.63 | wps 180.4 | ups 0.53 | wpb 338.8 | bsz 16.6 | num_updates 7000 | lr 2.26779e-05 | gnorm 7.8 | clip 100 | loss_scale 16 | train_wall 128 | gb_free 4.1 | wall 4225\n",
            "2025-05-03 10:08:18 | INFO | fairseq_cli.train | done training in 4196.0 seconds\n"
          ]
        }
      ],
      "source": [
        "# Finetuning the model\n",
        "\n",
        "# pls refer to fairseq documentaion to know more about each of these options (https://fairseq.readthedocs.io/en/latest/command_line_tools.html)\n",
        "\n",
        "\n",
        "# some notable args:\n",
        "# --max-update=1000     -> for this example, to demonstrate how to finetune we are only training for 1000 steps. You should increase this when finetuning\n",
        "# --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
        "# --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
        "# --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
        "# --restore-file        -> reload the pretrained checkpoint and start training from here (change this path for indic-en. Currently its is set to en-indic)\n",
        "# --reset-*             -> reset and not use lr scheduler, dataloader, optimizer etc of the older checkpoint\n",
        "# --max_tokns           -> this is max tokens per batch\n",
        "\n",
        "\n",
        "!( fairseq-train ../dataset/final_bin \\\n",
        "--max-source-positions=210 \\\n",
        "--max-target-positions=210 \\\n",
        "--max-update=7000 \\\n",
        "--save-interval=3 \\\n",
        "--arch=transformer_4x \\\n",
        "--criterion=label_smoothed_cross_entropy \\\n",
        "--source-lang=SRC \\\n",
        "--lr-scheduler=inverse_sqrt \\\n",
        "--target-lang=TGT \\\n",
        "--label-smoothing=0.1 \\\n",
        "--optimizer adam \\\n",
        "--adam-betas \"(0.9, 0.98)\" \\\n",
        "--clip-norm 1.0 \\\n",
        "--warmup-init-lr 1e-07 \\\n",
        "--warmup-updates 4000 \\\n",
        "--dropout 0.2 \\\n",
        "--tensorboard-logdir ../dataset/tensorboard-wandb \\\n",
        "--save-dir ../dataset/model \\\n",
        "--keep-last-epochs 1 \\\n",
        "--patience 5 \\\n",
        "--skip-invalid-size-inputs-valid-test \\\n",
        "--fp16 \\\n",
        "--user-dir model_configs \\\n",
        "--update-freq=2 \\\n",
        "--distributed-world-size 1 \\\n",
        "--max-tokens 256 \\\n",
        "--lr 3e-5 \\\n",
        "--restore-file ../indic-en/model/checkpoint_best.pt \\\n",
        "--reset-lr-scheduler \\\n",
        "--reset-meters \\\n",
        "--reset-dataloader \\\n",
        "--reset-optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash joint_translate.sh /content/finetuning/dataset/test/test.hi /content/finetuning/hi_bho_outputs.txt 'hi' 'bho' /content/finetuning/dataset"
      ],
      "metadata": {
        "id": "gaCLXqmdCBAJ",
        "outputId": "469a325c-4362-4aa0-ef2a-fe8d89cf8dde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 3 10:08:37 AM UTC 2025\n",
            "Applying normalization and script conversion\n",
            "100% 9/9 [00:00<00:00, 117.59it/s]\n",
            "Number of sentences in input: 9\n",
            "Applying BPE\n",
            "Decoding\n",
            "Extracting translations, script conversion and detokenization\n",
            "Translation completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/finetuning/dataset/model/checkpoint_best.pt /content/drive/MyDrive/model_bhoj"
      ],
      "metadata": {
        "id": "1r4dZXnwC_DB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpPsT1e7vuO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fe68d4-a19d-4733-cf63-121a696ea6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 9 10:06:33 AM UTC 2025\n",
            "Applying normalization and script conversion\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetuning/indicTrans/scripts/preprocess_translate.py\", line 172, in <module>\n",
            "    print(preprocess(infname, outfname, lang, transliterate))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/finetuning/indicTrans/scripts/preprocess_translate.py\", line 61, in preprocess\n",
            "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
            "                                  ^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/test/test.en'\n",
            "Number of sentences in input: \n",
            "Applying BPE\n",
            "joint_translate.sh: line 27: en_hi_outputs.txt.norm: No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetuning/indicTrans/scripts/add_tags_translate.py\", line 28, in <module>\n",
            "    with open(infname, 'r', encoding='utf-8') as infile, \\\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'en_hi_outputs.txt._bpe'\n",
            "Decoding\n",
            "Extracting translations, script conversion and detokenization\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/finetuning/indicTrans/scripts/postprocess_translate.py\", line 94, in <module>\n",
            "    input_size = int(sys.argv[3])\n",
            "                 ^^^^^^^^^^^^^^^^\n",
            "ValueError: invalid literal for int() with base 10: 'hi'\n",
            "Translation completed\n"
          ]
        }
      ],
      "source": [
        "# To test the models after training, you can use joint_translate.sh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# joint_translate takes src_file, output_fname, src_lang, tgt_lang, model_folder as inputs\n",
        "# src_file -> input text file to be translated\n",
        "# output_fname -> name of the output file (will get created) containing the model predictions\n",
        "# src_lang -> source lang code of the input text ( in this case we are using en-indic model and hence src_lang would be 'en')\n",
        "# tgt_lang -> target lang code of the input text ( tgt lang for en-indic model would be any of the 11 indic langs we trained on:\n",
        "#              as, bn, hi, gu, kn, ml, mr, or, pa, ta, te)\n",
        "# supported languages are:\n",
        "#              as - assamese, bn - bengali, gu - gujarathi, hi - hindi, kn - kannada,\n",
        "#              ml - malayalam, mr - marathi, or - oriya, pa - punjabi, ta - tamil, te - telugu\n",
        "\n",
        "# model_dir -> the directory containing the model and the vocab files\n",
        "\n",
        "# Note: if the translation is taking a lot of time, please tune the buffer_size and batch_size parameter for fairseq-interactive defined inside this joint_translate script\n",
        "\n",
        "\n",
        "# here we are translating the english sentences to hindi\n",
        "!bash joint_translate.sh $exp_dir/test/test.en en_hi_outputs.txt 'en' 'hi' $exp_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPqneByPxilN"
      },
      "outputs": [],
      "source": [
        "# to compute bleu scores for the predicitions with a reference file, use the following command\n",
        "# arguments:\n",
        "# pred_fname: file that contains model predictions\n",
        "# ref_fname: file that contains references\n",
        "# src_lang and tgt_lang : the source and target language\n",
        "\n",
        "bash compute_bleu.sh en_hi_outputs.txt $exp_dir/test/test.hi 'en' 'hi'\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "interpreter": {
      "hash": "3c7d4130300118f0c7487d576c6841c0dbbdeec039e1e658ac9b107412a09af0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}